{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5503c74d",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ee95b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T01:46:05.506376Z",
     "start_time": "2022-05-16T01:45:38.758203Z"
    },
    "id": "syvJ-zV-U54I"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import transformers as ppb  # pytorch transformers\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "import string\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import emoji\n",
    "import catboost as catboost\n",
    "from wordcloud import WordCloud\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a3e604",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c5b103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:32:37.599794Z",
     "start_time": "2022-05-08T06:32:35.340125Z"
    },
    "id": "wCeBWQD8TSu_"
   },
   "outputs": [],
   "source": [
    "sc = pd.read_csv(\"Suicide_Detection.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98a6e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:32:37.652241Z",
     "start_time": "2022-05-08T06:32:37.640309Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VbWyHK9aTSxW",
    "outputId": "8afe4f6b-7cd1-435f-fbd2-3e317140084e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class\n",
       "2  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
       "3  Am I weird I don't get affected by compliments...  non-suicide\n",
       "4  Finally 2020 is almost over... So I can never ...  non-suicide\n",
       "8          i need helpjust help me im crying so hard      suicide\n",
       "9  I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...      suicide"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68701e64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:32:37.755004Z",
     "start_time": "2022-05-08T06:32:37.680792Z"
    },
    "id": "vqSE4C3qA4u4"
   },
   "outputs": [],
   "source": [
    "sc['label'] = [1 if i == 'suicide' else 0 for i in sc['class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e004b",
   "metadata": {},
   "source": [
    "## Translate emoji to English discription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30753cd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:33:19.236151Z",
     "start_time": "2022-05-08T06:32:37.785368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "      <th>text_deemoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>1</td>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>1</td>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>1</td>\n",
       "      <td>I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348103</th>\n",
       "      <td>If you don't like rock then your not going to ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>If you don't like rock then your not going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348106</th>\n",
       "      <td>You how you can tell i have so many friends an...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>You how you can tell i have so many friends an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348107</th>\n",
       "      <td>pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>pee probably tastes like salty tea smirking_fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348108</th>\n",
       "      <td>The usual stuff you find hereI'm not posting t...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>1</td>\n",
       "      <td>The usual stuff you find hereI'm not posting t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348110</th>\n",
       "      <td>I still haven't beaten the first boss in Hollo...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>I still haven't beaten the first boss in Hollo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232074 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text        class  label  \\\n",
       "2       Ex Wife Threatening SuicideRecently I left my ...      suicide      1   \n",
       "3       Am I weird I don't get affected by compliments...  non-suicide      0   \n",
       "4       Finally 2020 is almost over... So I can never ...  non-suicide      0   \n",
       "8               i need helpjust help me im crying so hard      suicide      1   \n",
       "9       I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...      suicide      1   \n",
       "...                                                   ...          ...    ...   \n",
       "348103  If you don't like rock then your not going to ...  non-suicide      0   \n",
       "348106  You how you can tell i have so many friends an...  non-suicide      0   \n",
       "348107  pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...  non-suicide      0   \n",
       "348108  The usual stuff you find hereI'm not posting t...      suicide      1   \n",
       "348110  I still haven't beaten the first boss in Hollo...  non-suicide      0   \n",
       "\n",
       "                                             text_deemoji  \n",
       "2       Ex Wife Threatening SuicideRecently I left my ...  \n",
       "3       Am I weird I don't get affected by compliments...  \n",
       "4       Finally 2020 is almost over... So I can never ...  \n",
       "8               i need helpjust help me im crying so hard  \n",
       "9       I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...  \n",
       "...                                                   ...  \n",
       "348103  If you don't like rock then your not going to ...  \n",
       "348106  You how you can tell i have so many friends an...  \n",
       "348107  pee probably tastes like salty tea smirking_fa...  \n",
       "348108  The usual stuff you find hereI'm not posting t...  \n",
       "348110  I still haven't beaten the first boss in Hollo...  \n",
       "\n",
       "[232074 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emoji_change(str):\n",
    "    result = emoji.demojize(str, delimiters='  ')\n",
    "    return result\n",
    "\n",
    "\n",
    "sc['text_deemoji'] = sc['text'].apply(emoji_change)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3319b6",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee49b0a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:37:40.589430Z",
     "start_time": "2022-05-08T06:33:19.280261Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "FJcBPFxuA4yH",
    "outputId": "aafa7599-7c50-4db3-a3cc-f1f8613f4d89"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "      <th>text_deemoji</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>1</td>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>Ex Wife Threatening SuicideRecently left wife ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>weird n't get affected compliments coming some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>Finally 2020 almost ... never hear 2020 bad ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>1</td>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>need helpjust help im crying hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>1</td>\n",
       "      <td>I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...</td>\n",
       "      <td>‚Äô lostHello name Adam 16 ‚Äô struggling years ‚Äô ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348103</th>\n",
       "      <td>If you don't like rock then your not going to ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>If you don't like rock then your not going to ...</td>\n",
       "      <td>n't like rock going get anything go https //mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348106</th>\n",
       "      <td>You how you can tell i have so many friends an...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>You how you can tell i have so many friends an...</td>\n",
       "      <td>tell many friends lonely everything deprived ?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348107</th>\n",
       "      <td>pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>pee probably tastes like salty tea smirking_fa...</td>\n",
       "      <td>pee probably tastes like salty tea smirking_fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348108</th>\n",
       "      <td>The usual stuff you find hereI'm not posting t...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>1</td>\n",
       "      <td>The usual stuff you find hereI'm not posting t...</td>\n",
       "      <td>usual stuff find hereI 'm posting sympathy pit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348110</th>\n",
       "      <td>I still haven't beaten the first boss in Hollo...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "      <td>I still haven't beaten the first boss in Hollo...</td>\n",
       "      <td>still n't beaten first boss Hollow Knight 've ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232074 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text        class  label  \\\n",
       "2       Ex Wife Threatening SuicideRecently I left my ...      suicide      1   \n",
       "3       Am I weird I don't get affected by compliments...  non-suicide      0   \n",
       "4       Finally 2020 is almost over... So I can never ...  non-suicide      0   \n",
       "8               i need helpjust help me im crying so hard      suicide      1   \n",
       "9       I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...      suicide      1   \n",
       "...                                                   ...          ...    ...   \n",
       "348103  If you don't like rock then your not going to ...  non-suicide      0   \n",
       "348106  You how you can tell i have so many friends an...  non-suicide      0   \n",
       "348107  pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...  non-suicide      0   \n",
       "348108  The usual stuff you find hereI'm not posting t...      suicide      1   \n",
       "348110  I still haven't beaten the first boss in Hollo...  non-suicide      0   \n",
       "\n",
       "                                             text_deemoji  \\\n",
       "2       Ex Wife Threatening SuicideRecently I left my ...   \n",
       "3       Am I weird I don't get affected by compliments...   \n",
       "4       Finally 2020 is almost over... So I can never ...   \n",
       "8               i need helpjust help me im crying so hard   \n",
       "9       I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...   \n",
       "...                                                   ...   \n",
       "348103  If you don't like rock then your not going to ...   \n",
       "348106  You how you can tell i have so many friends an...   \n",
       "348107  pee probably tastes like salty tea smirking_fa...   \n",
       "348108  The usual stuff you find hereI'm not posting t...   \n",
       "348110  I still haven't beaten the first boss in Hollo...   \n",
       "\n",
       "                                                     Text  \n",
       "2       Ex Wife Threatening SuicideRecently left wife ...  \n",
       "3       weird n't get affected compliments coming some...  \n",
       "4       Finally 2020 almost ... never hear 2020 bad ye...  \n",
       "8                       need helpjust help im crying hard  \n",
       "9       ‚Äô lostHello name Adam 16 ‚Äô struggling years ‚Äô ...  \n",
       "...                                                   ...  \n",
       "348103  n't like rock going get anything go https //mu...  \n",
       "348106  tell many friends lonely everything deprived ?...  \n",
       "348107  pee probably tastes like salty tea smirking_fa...  \n",
       "348108  usual stuff find hereI 'm posting sympathy pit...  \n",
       "348110  still n't beaten first boss Hollow Knight 've ...  \n",
       "\n",
       "[232074 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop_words\n",
    "\n",
    "stop_words = set(\n",
    "    stopwords.words('english') +\n",
    "    [\".\", '.', \",\", \":\", \"''\", \"'s\", \"'\", \"``\", \"^\", \"(\", \")\", \"-\"])\n",
    "\n",
    "# Removed the stopwords\n",
    "stop_removed_list = []\n",
    "for line in sc['text_deemoji']:\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    stopword_removed = [\n",
    "        token for token in tokens if token.lower() not in stop_words\n",
    "    ]\n",
    "    removed_sent = (\" \").join(stopword_removed)\n",
    "    stop_removed_list.append(removed_sent)\n",
    "# append stopwords to a new column\n",
    "sc['Text'] = stop_removed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add6864a",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba04d4bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:37:52.426722Z",
     "start_time": "2022-05-08T06:37:43.538672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Regex\n",
    "\n",
    "\n",
    "def regex_clean(comment, ree, new):\n",
    "    line = re.sub(ree, new, comment, flags=re.IGNORECASE)\n",
    "    return line\n",
    "\n",
    "\n",
    "re1 = r\"\\_\"\n",
    "re2 = r\"filler\"\n",
    "re3 = r\"\\b(?:work[\\w]+|jobs?|career|intern(ship)?|position)\\b\"\n",
    "re4 = r\"\\b(?:co(\\-)?worker|interpersonal|managers?|boss|supervisor|colleague|employees?|staffs?|network)\\b\"\n",
    "sc['new_Text'] = sc['Text'].apply(regex_clean, ree=re1, new=' ')\n",
    "sc['new_Text'] = sc['new_Text'].apply(regex_clean, ree=re2, new=' ')\n",
    "sc['new_Text'] = sc['new_Text'].apply(regex_clean, ree=re3, new=' _WORK_ ')\n",
    "sc['new_Text'] = sc['new_Text'].apply(regex_clean,\n",
    "                                      ree=re4,\n",
    "                                      new=' _INTERPERSONAL_ ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708a140",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c656500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:53:09.179668Z",
     "start_time": "2022-05-08T06:37:55.432181Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrkAZAu8A41F",
    "outputId": "666e6030-c78e-47cb-cb63-fd297a0ccfe9"
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(\n",
    "        sentence.lower()))  # I add a lower() function here since it will\n",
    "    #tuple of (token, wordnet_tag)                                   # do a bad job with capital letters\n",
    "    wordnet_tagged = [(x[0], nltk_tag_to_wordnet_tag(x[1]))\n",
    "                      for x in nltk_tagged]\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "\n",
    "sc['lemm_Text'] = sc['new_Text'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ac5c8a",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90a5f5ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T02:05:54.288925Z",
     "start_time": "2022-05-16T02:05:54.187677Z"
    },
    "id": "nVx6-SPeL1Az"
   },
   "outputs": [],
   "source": [
    "news_corpus_df = sc\n",
    "news_corpus_df['text'] = sc['lemm_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "416a05bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T02:06:25.292298Z",
     "start_time": "2022-05-16T02:05:55.506991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Regex\n",
    "import re\n",
    "\n",
    "\n",
    "def regex_clean(comment, ree, new):\n",
    "    line = re.sub(ree, new, comment, flags=re.IGNORECASE)\n",
    "    return line\n",
    "\n",
    "\n",
    "# Mainly for removing '_' in the English discription after translating emojis\n",
    "re1 = r\"\\_\"\n",
    "\n",
    "# Remove noise words\n",
    "re2 = r\"fil(l|t)er\"\n",
    "re3 = r\"\\b(youtube)|(reddit)|(www)|(com)|(amp)|(webp)|(https)|(x200b)|(pjpg)|(format)|(png)|(auto)\\b\"\n",
    "\n",
    "# Translate oral language to written language\n",
    "re4 = r\"\\b(wan( )?na)\\b\"\n",
    "re5 = r\"\\b(gon( )?na)\\b\"\n",
    "\n",
    "news_corpus_df['text'] = news_corpus_df['text'].apply(regex_clean,\n",
    "                                                      ree=re1,\n",
    "                                                      new=' ')\n",
    "news_corpus_df['text'] = news_corpus_df['text'].apply(regex_clean,\n",
    "                                                      ree=re2,\n",
    "                                                      new=' ')\n",
    "news_corpus_df['text'] = news_corpus_df['text'].apply(regex_clean,\n",
    "                                                      ree=re3,\n",
    "                                                      new=' ')\n",
    "news_corpus_df['text'] = news_corpus_df['text'].apply(regex_clean,\n",
    "                                                      ree=re4,\n",
    "                                                      new=' want ')\n",
    "news_corpus_df['text'] = news_corpus_df['text'].apply(regex_clean,\n",
    "                                                      ree=re5,\n",
    "                                                      new=' go ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae468c6",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a90bfd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T02:11:07.505720Z",
     "start_time": "2022-05-16T02:10:41.492622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News TF-IDF: (232074, 45)\n",
      "   anymore want  best friend  ca anymore  dont know  dont want  end life  \\\n",
      "0           0.0          0.0         0.0        0.0        0.0       0.0   \n",
      "1           0.0          0.0         0.0        0.0        0.0       0.0   \n",
      "2           0.0          0.0         0.0        0.0        0.0       0.0   \n",
      "3           0.0          0.0         0.0        0.0        0.0       0.0   \n",
      "4           0.0          0.0         0.0        0.0        0.0       0.0   \n",
      "\n",
      "   family friends  feel bad  feel better  feel like  ...  want die  want end  \\\n",
      "0             0.0       0.0          0.0        0.0  ...       0.0       0.0   \n",
      "1             0.0       0.0          0.0        0.0  ...       0.0       0.0   \n",
      "2             0.0       0.0          0.0        0.0  ...       0.0       0.0   \n",
      "3             0.0       0.0          0.0        0.0  ...       0.0       0.0   \n",
      "4             0.0       0.0          0.0        0.0  ...       0.0       0.0   \n",
      "\n",
      "   want feel  want kill  want know  want live  want talk  year old  years ago  \\\n",
      "0        0.0        0.0        0.0        0.0        0.0       0.0        0.0   \n",
      "1        0.0        0.0        0.0        0.0        0.0       0.0        0.0   \n",
      "2        0.0        0.0        0.0        0.0        0.0       0.0        0.0   \n",
      "3        0.0        0.0        0.0        0.0        0.0       0.0        0.0   \n",
      "4        0.0        0.0        0.0        0.0        0.0       0.0        1.0   \n",
      "\n",
      "   years old  \n",
      "0        0.0  \n",
      "1        0.0  \n",
      "2        0.0  \n",
      "3        0.0  \n",
      "4        0.0  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "news_vectorizer = TfidfVectorizer(ngram_range=(2, 2),\n",
    "                                  min_df=0.01,\n",
    "                                  max_df=0.4,\n",
    "                                  stop_words=\"english\")\n",
    "\n",
    "X_news, news_terms = news_vectorizer.fit_transform(\n",
    "    news_corpus_df.text), news_vectorizer.get_feature_names_out()\n",
    "\n",
    "news_tf_idf = pd.DataFrame(X_news.toarray(), columns=news_terms)\n",
    "print(f\"News TF-IDF: {news_tf_idf.shape}\")\n",
    "print(news_tf_idf.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7843aa73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T02:19:16.104580Z",
     "start_time": "2022-05-16T02:19:15.344175Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yduanmu/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of X news is (232074, 45)\n",
      "Decomposed W news matrix is (232074, 2)\n",
      "Decomposed H news matrix is (2, 45)\n",
      "Topics:\n",
      "\n",
      "\n",
      "TOPIC 0\n",
      "\n",
      "\bfeel like (51.2%)\n",
      "\n",
      "\bfeels like (2.8%)\n",
      "\n",
      "\bhigh school (2.0%)\n",
      "\n",
      "\bfelt like (1.9%)\n",
      "\n",
      "\bmakes feel (1.9%)\n",
      "\n",
      "\bsuicidal thoughts (1.8%)\n",
      "\n",
      "\blike shit (1.7%)\n",
      "\n",
      "\byears ago (1.5%)\n",
      "\n",
      "\bbest friend (1.4%)\n",
      "\n",
      "\blong time (1.4%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 1\n",
      "\n",
      "\bwant die (38.0%)\n",
      "\n",
      "\breally want (4.9%)\n",
      "\n",
      "\bwant kill (3.3%)\n",
      "\n",
      "\bwant live (3.3%)\n",
      "\n",
      "\bfeels like (2.2%)\n",
      "\n",
      "\bsuicidal thoughts (2.2%)\n",
      "\n",
      "\bdont want (2.2%)\n",
      "\n",
      "\bwant end (2.2%)\n",
      "\n",
      "\bhigh school (2.1%)\n",
      "\n",
      "\byears ago (1.8%)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=2)\n",
    "W_news = nmf.fit_transform(X_news)\n",
    "H_news = nmf.components_\n",
    "print(f\"Original shape of X news is {X_news.shape}\")\n",
    "print(f\"Decomposed W news matrix is {W_news.shape}\")\n",
    "print(f\"Decomposed H news matrix is {H_news.shape}\")\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_top_tf_idf_tokens_for_topic(H: np.array,\n",
    "                                    feature_names: List[str],\n",
    "                                    num_top_tokens: int = 5):\n",
    "    \"\"\"\n",
    "  Uses the H matrix (K components x M original features) to identify for each\n",
    "  topic the most frequent tokens.\n",
    "  \"\"\"\n",
    "    for topic, vector in enumerate(H):\n",
    "        print(f\"TOPIC {topic}\\n\")\n",
    "        total = vector.sum()\n",
    "        top_scores = vector.argsort()[::-1][:num_top_tokens]\n",
    "        token_names = list(map(lambda idx: feature_names[idx], top_scores))\n",
    "        strengths = list(map(lambda idx: vector[idx] / total, top_scores))\n",
    "\n",
    "        for strength, token_name in zip(strengths, token_names):\n",
    "            print(f\"\\b{token_name} ({round(strength * 100, 1)}%)\\n\")\n",
    "        print(f\"=\" * 50)\n",
    "\n",
    "\n",
    "print(f\"Topics:\\n\\n\")\n",
    "get_top_tf_idf_tokens_for_topic(H_news, news_tf_idf.columns.tolist(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "111c1887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T02:19:19.797055Z",
     "start_time": "2022-05-16T02:19:19.789633Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_top_documents_for_each_topic(W: np.array,\n",
    "                                     documents: List[str],\n",
    "                                     num_docs: int = 5):\n",
    "    sorted_docs = W.argsort(axis=0)[::-1]\n",
    "    top_docs = sorted_docs[:num_docs].T\n",
    "    per_document_totals = W.sum(axis=1)\n",
    "    for topic, top_documents_for_topic in enumerate(top_docs):\n",
    "        print(f\"Topic {topic}\")\n",
    "        for doc in top_documents_for_topic:\n",
    "            score = W[doc][topic]\n",
    "            percent_about_topic = round(score / per_document_totals[doc] * 100,\n",
    "                                        1)\n",
    "            print(f\"{percent_about_topic}%\", documents[doc])\n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71509529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T02:19:20.325062Z",
     "start_time": "2022-05-16T02:19:20.280679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "100.0% 'm 16 feel like girlfriend might pregnant . Please help me.I ca n't handle father 16 , 'll ruin life feel like one way . 've using protection period almost 3 weeks late . Edit : 'm reading replies appreciate , thank . 're helping .\n",
      "100.0% Spent day tearsThis way live need . n't feel like anything live .\n",
      "100.0% Idk ‚Äô even writing thisI need say . ‚Äô feeling low long feel like stop . ‚Äô scared , course ‚Äô stand anymore . many people many worse problems ‚Äô tried hard accept life live seriously . head eternal fog , ‚Äô find motivation anything every single day . ‚Äô burden ‚Äô making everyone around unhappy ‚Äô really ‚Äô stand . want stop . ‚Äô  e give advice looked today overwhelmed . snapped . ‚Äô first attempt know regretted ‚Äô convince ‚Äô happy anymore ‚Äô . , ‚Äô made deal , 28 days nothing gets better ‚Äô . ‚Äô  forting strangest way . guess ‚Äô knowing things better either way . ‚Äô even know ‚Äô writing honestly guess need someone listen maybe . ‚Äô tired .\n",
      "100.0% 's half 2 morning want get something chest head bursting.Throwaway , bullets 'm lazy 's  plicated -18 yo female -Been bf 1y10m -bf history sexting facebook ( found bra photo girl ) -he deleted account ages ago -reactivated account recently -Terrified , seen flirty conversation girl goes college dug back dark feelings . -Failed a-levels -Did n't make uni -Working shitty fast food minimum wage job -Not enough money move parents house/get driving licence -Feel like life going -Posting feel nowhere else turn . Edit : forgot mention 0 friends .\n",
      "100.0% feel like dying Ya know , forever sleep feel numb outside world want pain stop . feel like nuisance world , know life ups downs feel like ‚Äô forever never go back\n",
      "==================================================\n",
      "Topic 1\n",
      "100.0% 'm turning 18 soon 'm fuckkmg terrified might attracted kidsplease please please n't fuckinf want n't know else post . 'm 17 turn 18 month n't want hurt people 'm panicking fucking badly n't want fuck children n't want hurt people n't want feel way n't feel fucking weird want die want die want die want dje want die want die please god fucking help fucking kill n't want n't fucking want ca n't breathe everything feels fucking hot feels like face burning want die want die want die want die want die happening fucking fucking please god fucking kill want fucking die right fucking n't want hurt anyone n't want hurt anyoje dont want yo dhurt anyone dont watnto hurt anyone want die want die want die want die want die want die 'm fucking shaking want die want die want die\n",
      "100.0% ‚Äô  go  die sooner laterMaybe even tomorrow . want die . Depressed 24/7 , Suicidal , Cutting , average 16M Gym guy . ‚Äô post maybe someone talk little numb pain .\n",
      "100.0% dont think much longer im past highly suicidal times quit job cos struggling work mental illnesses , dyslexia likely adhd according psychiatrists also bad general anxiety dont job in e im 19 quit every friendship becuase felt stupid , family Christians second half siblings ashamed think im dropout would often make jokes suicidal dont deal often married , im stuck home in e spend time playing video games struggle sleep keep normal sleep routine soon money run run money soon im getting lower lower , every high  es incredibly low low highs far inbertween want die wanted try create in e anxiety makes seriously hard would guys ideas\n",
      "100.0% ‚Äô afford therapy anymore . Insurance sucks . Extreme guilt anxiety . ‚Äô ... done.I ‚Äô . ‚Äô . quit therapy . barely even afford medication . ‚Äô afraid even make another suicide attempt fail ‚Äô hospitalized ‚Äô able afford bill either . want slap younger face . ‚Äô wrecked guilt . ‚Äô probably next person get called ‚Äú cancelled ‚Äù god want die . wasted entire summer . ‚Äô anything . ‚Äô friends anything . ‚Äô outsider looking ‚Äô relate anyone . ‚Äô even go social media ‚Äô intensely jealous people . job sit nothing day . ‚Äô new projects months . think boss forgot even work . ‚Äô body desk . want die . want dead . Nothing gets better . Summer fucking wasted . hate .\n",
      "100.0% ‚Äô even make post explaining want die , many reasons.That ‚Äô feel right . ‚Äô love vent get , ‚Äô much ‚Äô exhausting . Fuck everything .\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "get_top_documents_for_each_topic(W_news,\n",
    "                                 news_corpus_df.text.tolist(),\n",
    "                                 num_docs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4fc9c",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43476102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T02:19:55.271007Z",
     "start_time": "2022-05-16T02:19:24.490616Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "oWL3hhZoFQkY",
    "outputId": "2bd3f583-2d10-40f2-f50a-917e1472d4be",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News TF-IDF: (232074, 28020)\n",
      "   000 000 000  000 000 billion  000 000 million  000 000 quadrillion  \\\n",
      "0          0.0              0.0              0.0                  0.0   \n",
      "1          0.0              0.0              0.0                  0.0   \n",
      "2          0.0              0.0              0.0                  0.0   \n",
      "3          0.0              0.0              0.0                  0.0   \n",
      "4          0.0              0.0              0.0                  0.0   \n",
      "\n",
      "   000 000 trillion  000 billion day  000 miles away  000 million day  \\\n",
      "0               0.0              0.0             0.0              0.0   \n",
      "1               0.0              0.0             0.0              0.0   \n",
      "2               0.0              0.0             0.0              0.0   \n",
      "3               0.0              0.0             0.0              0.0   \n",
      "4               0.0              0.0             0.0              0.0   \n",
      "\n",
      "   000 quadrillion day  000 student loans  ...  yr old female  yr old girl  \\\n",
      "0                  0.0                0.0  ...            0.0          0.0   \n",
      "1                  0.0                0.0  ...            0.0          0.0   \n",
      "2                  0.0                0.0  ...            0.0          0.0   \n",
      "3                  0.0                0.0  ...            0.0          0.0   \n",
      "4                  0.0                0.0  ...            0.0          0.0   \n",
      "\n",
      "   yr old male  yu gi oh  yxcvbnm day 172  zany face face  zany face zany  \\\n",
      "0          0.0       0.0              0.0             0.0             0.0   \n",
      "1          0.0       0.0              0.0             0.0             0.0   \n",
      "2          0.0       0.0              0.0             0.0             0.0   \n",
      "3          0.0       0.0              0.0             0.0             0.0   \n",
      "4          0.0       0.0              0.0             0.0             0.0   \n",
      "\n",
      "   zero self esteem  zero social skills  zipper mouth face  \n",
      "0               0.0                 0.0                0.0  \n",
      "1               0.0                 0.0                0.0  \n",
      "2               0.0                 0.0                0.0  \n",
      "3               0.0                 0.0                0.0  \n",
      "4               0.0                 0.0                0.0  \n",
      "\n",
      "[5 rows x 28020 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "news_vectorizer = TfidfVectorizer(ngram_range=(3, 3),\n",
    "                                  min_df=10,\n",
    "                                  max_df=0.4,\n",
    "                                  stop_words=\"english\")\n",
    "\n",
    "X_news, news_terms = news_vectorizer.fit_transform(\n",
    "    news_corpus_df.text), news_vectorizer.get_feature_names_out()\n",
    "\n",
    "news_tf_idf = pd.DataFrame(X_news.toarray(), columns=news_terms)\n",
    "print(f\"News TF-IDF: {news_tf_idf.shape}\")\n",
    "print(news_tf_idf.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd677c75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T02:20:00.133613Z",
     "start_time": "2022-05-16T02:19:59.199466Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3qf86a1tBsw",
    "outputId": "16000a61-b8c4-4ea2-ee8f-3d5075bdf0b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yduanmu/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of X news is (232074, 28020)\n",
      "Decomposed W news matrix is (232074, 2)\n",
      "Decomposed H news matrix is (2, 28020)\n"
     ]
    }
   ],
   "source": [
    "W_news = nmf.fit_transform(X_news)\n",
    "H_news = nmf.components_\n",
    "print(f\"Original shape of X news is {X_news.shape}\")\n",
    "print(f\"Decomposed W news matrix is {W_news.shape}\")\n",
    "print(f\"Decomposed H news matrix is {H_news.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e8f94d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T02:20:01.742564Z",
     "start_time": "2022-05-16T02:20:00.945643Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unctGPrXPLp7",
    "outputId": "f3314cec-7dde-4651-bc7d-d6009311eebb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "\n",
      "\n",
      "TOPIC 0\n",
      "\n",
      "\bsmiling face sunglasses (57.3%)\n",
      "\n",
      "\bsunglasses smiling face (7.7%)\n",
      "\n",
      "\bface sunglasses smiling (7.7%)\n",
      "\n",
      "\bcool smiling face (1.4%)\n",
      "\n",
      "\bface tears joy (0.7%)\n",
      "\n",
      "\bface smiling face (0.6%)\n",
      "\n",
      "\bface steam nose (0.6%)\n",
      "\n",
      "\bbackhand index pointing (0.6%)\n",
      "\n",
      "\bface sunglasses thumbs (0.5%)\n",
      "\n",
      "\bshit smiling face (0.4%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 1\n",
      "\n",
      "\bloudly crying face (42.3%)\n",
      "\n",
      "\bface loudly crying (11.5%)\n",
      "\n",
      "\bcrying face loudly (10.6%)\n",
      "\n",
      "\bface tears joy (0.8%)\n",
      "\n",
      "\bface pensive face (0.5%)\n",
      "\n",
      "\bface rolling eyes (0.5%)\n",
      "\n",
      "\bface water pistol (0.4%)\n",
      "\n",
      "\bcrying face want (0.4%)\n",
      "\n",
      "\bface steam nose (0.4%)\n",
      "\n",
      "\bpensive face pensive (0.4%)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_top_tf_idf_tokens_for_topic(H: np.array,\n",
    "                                    feature_names: List[str],\n",
    "                                    num_top_tokens: int = 5):\n",
    "    \"\"\"\n",
    "  Uses the H matrix (K components x M original features) to identify for each\n",
    "  topic the most frequent tokens.\n",
    "  \"\"\"\n",
    "    for topic, vector in enumerate(H):\n",
    "        print(f\"TOPIC {topic}\\n\")\n",
    "        total = vector.sum()\n",
    "        top_scores = vector.argsort()[::-1][:num_top_tokens]\n",
    "        token_names = list(map(lambda idx: feature_names[idx], top_scores))\n",
    "        strengths = list(map(lambda idx: vector[idx] / total, top_scores))\n",
    "\n",
    "        for strength, token_name in zip(strengths, token_names):\n",
    "            print(f\"\\b{token_name} ({round(strength * 100, 1)}%)\\n\")\n",
    "        print(f\"=\" * 50)\n",
    "\n",
    "\n",
    "print(f\"Topics:\\n\\n\")\n",
    "get_top_tf_idf_tokens_for_topic(H_news, news_tf_idf.columns.tolist(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c58cb8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T02:07:35.425836Z",
     "start_time": "2022-05-16T02:07:35.371400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIFXsYHHuQmw",
    "outputId": "dec15820-74b3-4724-b972-d503aeedc7f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "100.0% Ive failed simp September Ive failed gf thats nice think smiling face with sunglasses moai moai smiling face with sunglasses moai smiling face with sunglasses smiling face with sunglasses moai smiling face with sunglasses\n",
      "100.0% Guys got PTSD pensive face got Potential suck dick fire smiling face with sunglasses hundred points hundred points\n",
      "100.0% u didnt buy bobux ur  go  die didnt buy bobux guys hold dont gf buy bobux smiling face with sunglasses\n",
      "100.0% heard ‚Äô smart guys work ijk plane daily smiling face with sunglasses\n",
      "100.0% might be e male stripper Imagine , get thousand money EVERY NIGHT . social circle pretty big , get lots sex thots , stripping shirt smiling face with sunglasses\n",
      "==================================================\n",
      "Topic 1\n",
      "100.0% total bruh moment loudly crying face grandpa uses voice text thing send messages mom saying something like ‚Äú hate pictures ‚Äù phone caught ‚Äú sounds good ‚Äù , ‚Äú hate ‚Äù . text got sent best friend sent back ‚Äú hate , wtf ‚Äù loudly crying face loudly crying face mom call clear loudly crying face loudly crying face\n",
      "100.0% met bf first time 6 months ran himafter hugging without saying good bye . OMFG ‚Äô EMBARRASSED , WANT FUCKING DIE loudly crying face loudly crying face loudly crying face friend ( ‚Äô also bf friend ) hanging 2 hours , ‚Äô hang longer bf going birthday party . came meeting boyfriend , ‚Äô meeting bf ‚Äô know birthday party . met first time meeting 6 months , AWKWARD ASF loudly crying face loudly crying face ‚Äô say hi , ‚Äô look listening friend talk loudly crying face go , said ¬´ hug ? ¬ª bf , ‚Äô say anything looked shy . ran hug 10 seconds , said ¬´ cozy ¬ª hugging . also hugged friend right , hugged friend ran away without saying goodbye loudly crying face loudly crying face FUCKING AWKWARD loudly crying face RUNNING AWAY HEARD FRIEND LAUGHING loudly crying face loudly crying face loudly crying face loudly crying face ran embarrassing loudly crying face idk loudly crying face\n",
      "100.0% ‚Äô 14 deep ... History like story ... loudly crying face loudly crying face\n",
      "100.0% r cute boys cant find guys wtf.. even exist ... loudly crying face loudly crying face\n",
      "100.0% Fjdjdjjd im mad , , ‚Äô contact miss fucking much tkfiixsowknenrjf , , good moving loudly crying face loudly crying face\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "get_top_documents_for_each_topic(W_news,\n",
    "                                 news_corpus_df.text.tolist(),\n",
    "                                 num_docs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6799f",
   "metadata": {},
   "source": [
    "# Model Exploration - Logistic (Baseline), Random Forest, LightGBM, XGBoost, CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb63e06",
   "metadata": {},
   "source": [
    "## Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "651a3c04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:53:15.855531Z",
     "start_time": "2022-05-08T06:53:15.853233Z"
    },
    "id": "1xuxbq3dUkc9"
   },
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (BertModel, BertTokenizer,\n",
    "                                                    'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce9eeb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:53:25.389819Z",
     "start_time": "2022-05-08T06:53:18.828238Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zahAl-O1Ukfv",
    "outputId": "f7ee99fe-b842-42e4-8b09-3685f792b196"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights,\n",
    "                                            do_lower_case=True)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feb9fc65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:59:13.549241Z",
     "start_time": "2022-05-08T06:53:28.464732Z"
    },
    "id": "HimqYUvLUkir"
   },
   "outputs": [],
   "source": [
    "tokenized = sc['lemm_Text'].apply((lambda x: tokenizer.encode(\n",
    "    x, add_special_tokens=True, padding=True, truncation=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fc23877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:59:30.547496Z",
     "start_time": "2022-05-08T06:59:19.559428Z"
    },
    "id": "ePqG_BtsHL6c"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31368800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:59:34.206185Z",
     "start_time": "2022-05-08T06:59:33.530711Z"
    },
    "id": "G1CTDjqxJcoC"
   },
   "outputs": [],
   "source": [
    "feature_data = pd.DataFrame(np.array(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a001add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:59:37.941306Z",
     "start_time": "2022-05-08T06:59:37.929334Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "E1G3Lng4Jcrv",
    "outputId": "4b85a61d-b998-4a2a-c2fa-ea76165e0543"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>4654</td>\n",
       "      <td>2564</td>\n",
       "      <td>15686</td>\n",
       "      <td>5920</td>\n",
       "      <td>2890</td>\n",
       "      <td>13013</td>\n",
       "      <td>2135</td>\n",
       "      <td>2681</td>\n",
       "      <td>2564</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>6881</td>\n",
       "      <td>1050</td>\n",
       "      <td>1005</td>\n",
       "      <td>1056</td>\n",
       "      <td>2131</td>\n",
       "      <td>5360</td>\n",
       "      <td>19394</td>\n",
       "      <td>2272</td>\n",
       "      <td>2619</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2633</td>\n",
       "      <td>12609</td>\n",
       "      <td>2471</td>\n",
       "      <td>1012</td>\n",
       "      <td>1012</td>\n",
       "      <td>1012</td>\n",
       "      <td>2196</td>\n",
       "      <td>2963</td>\n",
       "      <td>12609</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>2342</td>\n",
       "      <td>2393</td>\n",
       "      <td>29427</td>\n",
       "      <td>2393</td>\n",
       "      <td>10047</td>\n",
       "      <td>5390</td>\n",
       "      <td>2524</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>1521</td>\n",
       "      <td>2439</td>\n",
       "      <td>18223</td>\n",
       "      <td>2080</td>\n",
       "      <td>2171</td>\n",
       "      <td>4205</td>\n",
       "      <td>2385</td>\n",
       "      <td>1521</td>\n",
       "      <td>5998</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1      2      3     4      5      6      7     8      9    ...  502  \\\n",
       "0  101  4654   2564  15686  5920   2890  13013   2135  2681   2564  ...    0   \n",
       "1  101  6881   1050   1005  1056   2131   5360  19394  2272   2619  ...    0   \n",
       "2  101  2633  12609   2471  1012   1012   1012   2196  2963  12609  ...    0   \n",
       "3  101  2342   2393  29427  2393  10047   5390   2524   102      0  ...    0   \n",
       "4  101  1521   2439  18223  2080   2171   4205   2385  1521   5998  ...    0   \n",
       "\n",
       "   503  504  505  506  507  508  509  510  511  \n",
       "0    0    0    0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 512 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaa5536c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:59:41.970148Z",
     "start_time": "2022-05-08T06:59:41.356465Z"
    }
   },
   "outputs": [],
   "source": [
    "scsc = sc.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0af5272",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T07:01:08.333668Z",
     "start_time": "2022-05-08T07:01:02.828191Z"
    },
    "id": "A9WeBGS7K5-s"
   },
   "outputs": [],
   "source": [
    "model_data = feature_data.join(sc)\n",
    "model_data.drop([\n",
    "    'text_deemoji', 'text', 'class', 'Text', 'lemm_Text', 'new_Text', 'index'\n",
    "],\n",
    "                axis=1,\n",
    "                inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48583056",
   "metadata": {},
   "source": [
    "## Train/Test Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e80972b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T07:01:32.480917Z",
     "start_time": "2022-05-08T07:01:31.962652Z"
    },
    "id": "6DpxJyXNK6S_"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(model_data.loc[:, [i for i in range(0, 512)]])\n",
    "Y = pd.DataFrame(model_data.loc[:, 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b467ec",
   "metadata": {},
   "source": [
    "## Logistic (Baseline) - Accuracy 71.8% (7.21s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7773bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinson/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/vinson/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27185  7687]\n",
      " [11968 22783]]\n",
      "0.7176938655329417\n",
      "CPU times: user 26 s, sys: 3.96 s, total: 30 s\n",
      "Wall time: 7.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_trn, X_tst, Y_trn, Y_tst = train_test_split(X, Y, test_size=.3)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_trn, Y_trn)\n",
    "y_pred = model.predict(X_tst)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(Y_tst, y_pred))\n",
    "print(accuracy_score(Y_tst, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f1b30",
   "metadata": {},
   "source": [
    "## Random Forest - Accuracy 77.1% (1 min 31 s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c1d9c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27420  7433]\n",
      " [ 8531 26239]]\n",
      "0.7707079557042931\n",
      "CPU times: user 1min 28s, sys: 1.55 s, total: 1min 29s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#RF\n",
    "\n",
    "X_trn, X_tst, Y_trn, Y_tst = train_test_split(X, Y, test_size=.3)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_trn, Y_trn)\n",
    "y_pred = model.predict(X_tst)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(Y_tst, y_pred))\n",
    "print(accuracy_score(Y_tst, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bdc098",
   "metadata": {},
   "source": [
    "## LightGBM - Accuracy 82.2% (8.45s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0eec8b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinson/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/vinson/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29223  5557]\n",
      " [ 6813 28030]]\n",
      "0.8223288281171452\n",
      "CPU times: user 41.3 s, sys: 4.62 s, total: 45.9 s\n",
      "Wall time: 8.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#LGBM\n",
    "\n",
    "X_trn, X_tst, Y_trn, Y_tst = train_test_split(X, Y, test_size=.3)\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "model.fit(X_trn, Y_trn)\n",
    "y_pred = model.predict(X_tst)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(Y_tst, y_pred))\n",
    "print(accuracy_score(Y_tst, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb7247",
   "metadata": {},
   "source": [
    "## XGBoost - Accuracy 85.7% (1 min 40 s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7be87c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinson/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/vinson/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/vinson/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[30243  4361]\n",
      " [ 5569 29450]]\n",
      "0.8573747181247576\n",
      "CPU times: user 11min 30s, sys: 27.1 s, total: 11min 57s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#XGB\n",
    "\n",
    "X_trn, X_tst, Y_trn, Y_tst = train_test_split(X, Y, test_size=.3)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_trn, Y_trn)\n",
    "y_pred = model.predict(X_tst)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(Y_tst, y_pred))\n",
    "print(accuracy_score(Y_tst, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027438c4",
   "metadata": {},
   "source": [
    "## CatBoost - Accuracy 85.7% (1 min 45 s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c594c0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.090555\n",
      "0:\tlearn: 0.6660412\ttotal: 201ms\tremaining: 3m 21s\n",
      "1:\tlearn: 0.6440649\ttotal: 319ms\tremaining: 2m 39s\n",
      "2:\tlearn: 0.6255121\ttotal: 449ms\tremaining: 2m 29s\n",
      "3:\tlearn: 0.6109721\ttotal: 574ms\tremaining: 2m 22s\n",
      "4:\tlearn: 0.5981821\ttotal: 702ms\tremaining: 2m 19s\n",
      "5:\tlearn: 0.5860336\ttotal: 822ms\tremaining: 2m 16s\n",
      "6:\tlearn: 0.5771437\ttotal: 948ms\tremaining: 2m 14s\n",
      "7:\tlearn: 0.5698121\ttotal: 1.06s\tremaining: 2m 11s\n",
      "8:\tlearn: 0.5626728\ttotal: 1.18s\tremaining: 2m 10s\n",
      "9:\tlearn: 0.5575743\ttotal: 1.3s\tremaining: 2m 9s\n",
      "10:\tlearn: 0.5526531\ttotal: 1.42s\tremaining: 2m 8s\n",
      "11:\tlearn: 0.5457297\ttotal: 1.54s\tremaining: 2m 6s\n",
      "12:\tlearn: 0.5416407\ttotal: 1.65s\tremaining: 2m 5s\n",
      "13:\tlearn: 0.5376438\ttotal: 1.76s\tremaining: 2m 3s\n",
      "14:\tlearn: 0.5344963\ttotal: 1.89s\tremaining: 2m 3s\n",
      "15:\tlearn: 0.5318715\ttotal: 2.01s\tremaining: 2m 3s\n",
      "16:\tlearn: 0.5286555\ttotal: 2.14s\tremaining: 2m 3s\n",
      "17:\tlearn: 0.5265897\ttotal: 2.27s\tremaining: 2m 3s\n",
      "18:\tlearn: 0.5247753\ttotal: 2.39s\tremaining: 2m 3s\n",
      "19:\tlearn: 0.5229843\ttotal: 2.49s\tremaining: 2m 2s\n",
      "20:\tlearn: 0.5214919\ttotal: 2.62s\tremaining: 2m 1s\n",
      "21:\tlearn: 0.5202118\ttotal: 2.72s\tremaining: 2m 1s\n",
      "22:\tlearn: 0.5187673\ttotal: 2.82s\tremaining: 2m\n",
      "23:\tlearn: 0.5153730\ttotal: 2.94s\tremaining: 1m 59s\n",
      "24:\tlearn: 0.5142463\ttotal: 3.05s\tremaining: 1m 59s\n",
      "25:\tlearn: 0.5125928\ttotal: 3.16s\tremaining: 1m 58s\n",
      "26:\tlearn: 0.5114554\ttotal: 3.27s\tremaining: 1m 57s\n",
      "27:\tlearn: 0.5099089\ttotal: 3.37s\tremaining: 1m 57s\n",
      "28:\tlearn: 0.5089310\ttotal: 3.48s\tremaining: 1m 56s\n",
      "29:\tlearn: 0.5063221\ttotal: 3.59s\tremaining: 1m 56s\n",
      "30:\tlearn: 0.5051971\ttotal: 3.68s\tremaining: 1m 55s\n",
      "31:\tlearn: 0.5043025\ttotal: 3.79s\tremaining: 1m 54s\n",
      "32:\tlearn: 0.5033174\ttotal: 3.89s\tremaining: 1m 53s\n",
      "33:\tlearn: 0.5020657\ttotal: 3.99s\tremaining: 1m 53s\n",
      "34:\tlearn: 0.5003084\ttotal: 4.08s\tremaining: 1m 52s\n",
      "35:\tlearn: 0.4994294\ttotal: 4.17s\tremaining: 1m 51s\n",
      "36:\tlearn: 0.4985329\ttotal: 4.28s\tremaining: 1m 51s\n",
      "37:\tlearn: 0.4960908\ttotal: 4.38s\tremaining: 1m 50s\n",
      "38:\tlearn: 0.4955242\ttotal: 4.48s\tremaining: 1m 50s\n",
      "39:\tlearn: 0.4943956\ttotal: 4.59s\tremaining: 1m 50s\n",
      "40:\tlearn: 0.4924963\ttotal: 4.7s\tremaining: 1m 50s\n",
      "41:\tlearn: 0.4918870\ttotal: 4.81s\tremaining: 1m 49s\n",
      "42:\tlearn: 0.4906239\ttotal: 4.91s\tremaining: 1m 49s\n",
      "43:\tlearn: 0.4889717\ttotal: 5.01s\tremaining: 1m 48s\n",
      "44:\tlearn: 0.4878009\ttotal: 5.11s\tremaining: 1m 48s\n",
      "45:\tlearn: 0.4871724\ttotal: 5.22s\tremaining: 1m 48s\n",
      "46:\tlearn: 0.4866748\ttotal: 5.32s\tremaining: 1m 47s\n",
      "47:\tlearn: 0.4859405\ttotal: 5.42s\tremaining: 1m 47s\n",
      "48:\tlearn: 0.4853393\ttotal: 5.51s\tremaining: 1m 47s\n",
      "49:\tlearn: 0.4846702\ttotal: 5.62s\tremaining: 1m 46s\n",
      "50:\tlearn: 0.4837529\ttotal: 5.72s\tremaining: 1m 46s\n",
      "51:\tlearn: 0.4831160\ttotal: 5.82s\tremaining: 1m 46s\n",
      "52:\tlearn: 0.4826357\ttotal: 5.94s\tremaining: 1m 46s\n",
      "53:\tlearn: 0.4819151\ttotal: 6.05s\tremaining: 1m 45s\n",
      "54:\tlearn: 0.4808659\ttotal: 6.13s\tremaining: 1m 45s\n",
      "55:\tlearn: 0.4788502\ttotal: 6.24s\tremaining: 1m 45s\n",
      "56:\tlearn: 0.4772388\ttotal: 6.34s\tremaining: 1m 44s\n",
      "57:\tlearn: 0.4767568\ttotal: 6.46s\tremaining: 1m 44s\n",
      "58:\tlearn: 0.4762596\ttotal: 6.55s\tremaining: 1m 44s\n",
      "59:\tlearn: 0.4757433\ttotal: 6.66s\tremaining: 1m 44s\n",
      "60:\tlearn: 0.4752363\ttotal: 6.78s\tremaining: 1m 44s\n",
      "61:\tlearn: 0.4748279\ttotal: 6.89s\tremaining: 1m 44s\n",
      "62:\tlearn: 0.4742792\ttotal: 7s\tremaining: 1m 44s\n",
      "63:\tlearn: 0.4734809\ttotal: 7.11s\tremaining: 1m 44s\n",
      "64:\tlearn: 0.4731632\ttotal: 7.21s\tremaining: 1m 43s\n",
      "65:\tlearn: 0.4728266\ttotal: 7.3s\tremaining: 1m 43s\n",
      "66:\tlearn: 0.4717980\ttotal: 7.42s\tremaining: 1m 43s\n",
      "67:\tlearn: 0.4714140\ttotal: 7.52s\tremaining: 1m 43s\n",
      "68:\tlearn: 0.4710745\ttotal: 7.63s\tremaining: 1m 42s\n",
      "69:\tlearn: 0.4706076\ttotal: 7.73s\tremaining: 1m 42s\n",
      "70:\tlearn: 0.4699201\ttotal: 7.84s\tremaining: 1m 42s\n",
      "71:\tlearn: 0.4695692\ttotal: 7.94s\tremaining: 1m 42s\n",
      "72:\tlearn: 0.4680282\ttotal: 8.05s\tremaining: 1m 42s\n",
      "73:\tlearn: 0.4677047\ttotal: 8.15s\tremaining: 1m 42s\n",
      "74:\tlearn: 0.4673985\ttotal: 8.26s\tremaining: 1m 41s\n",
      "75:\tlearn: 0.4671646\ttotal: 8.35s\tremaining: 1m 41s\n",
      "76:\tlearn: 0.4665983\ttotal: 8.45s\tremaining: 1m 41s\n",
      "77:\tlearn: 0.4658474\ttotal: 8.54s\tremaining: 1m 40s\n",
      "78:\tlearn: 0.4656028\ttotal: 8.63s\tremaining: 1m 40s\n",
      "79:\tlearn: 0.4647167\ttotal: 8.72s\tremaining: 1m 40s\n",
      "80:\tlearn: 0.4643818\ttotal: 8.84s\tremaining: 1m 40s\n",
      "81:\tlearn: 0.4635775\ttotal: 8.95s\tremaining: 1m 40s\n",
      "82:\tlearn: 0.4631944\ttotal: 9.03s\tremaining: 1m 39s\n",
      "83:\tlearn: 0.4629419\ttotal: 9.13s\tremaining: 1m 39s\n",
      "84:\tlearn: 0.4626685\ttotal: 9.22s\tremaining: 1m 39s\n",
      "85:\tlearn: 0.4615378\ttotal: 9.3s\tremaining: 1m 38s\n",
      "86:\tlearn: 0.4607941\ttotal: 9.42s\tremaining: 1m 38s\n",
      "87:\tlearn: 0.4604633\ttotal: 9.54s\tremaining: 1m 38s\n",
      "88:\tlearn: 0.4601636\ttotal: 9.65s\tremaining: 1m 38s\n",
      "89:\tlearn: 0.4598724\ttotal: 9.73s\tremaining: 1m 38s\n",
      "90:\tlearn: 0.4594821\ttotal: 9.83s\tremaining: 1m 38s\n",
      "91:\tlearn: 0.4592491\ttotal: 9.91s\tremaining: 1m 37s\n",
      "92:\tlearn: 0.4589428\ttotal: 10s\tremaining: 1m 37s\n",
      "93:\tlearn: 0.4585010\ttotal: 10.1s\tremaining: 1m 37s\n",
      "94:\tlearn: 0.4582835\ttotal: 10.2s\tremaining: 1m 37s\n",
      "95:\tlearn: 0.4579619\ttotal: 10.3s\tremaining: 1m 37s\n",
      "96:\tlearn: 0.4570012\ttotal: 10.4s\tremaining: 1m 37s\n",
      "97:\tlearn: 0.4568411\ttotal: 10.5s\tremaining: 1m 36s\n",
      "98:\tlearn: 0.4565197\ttotal: 10.6s\tremaining: 1m 36s\n",
      "99:\tlearn: 0.4563375\ttotal: 10.7s\tremaining: 1m 36s\n",
      "100:\tlearn: 0.4559518\ttotal: 10.8s\tremaining: 1m 36s\n",
      "101:\tlearn: 0.4556594\ttotal: 10.9s\tremaining: 1m 36s\n",
      "102:\tlearn: 0.4553713\ttotal: 11s\tremaining: 1m 35s\n",
      "103:\tlearn: 0.4550720\ttotal: 11.1s\tremaining: 1m 35s\n",
      "104:\tlearn: 0.4547820\ttotal: 11.2s\tremaining: 1m 35s\n",
      "105:\tlearn: 0.4545846\ttotal: 11.3s\tremaining: 1m 35s\n",
      "106:\tlearn: 0.4543809\ttotal: 11.4s\tremaining: 1m 35s\n",
      "107:\tlearn: 0.4539910\ttotal: 11.5s\tremaining: 1m 35s\n",
      "108:\tlearn: 0.4532018\ttotal: 11.6s\tremaining: 1m 34s\n",
      "109:\tlearn: 0.4527815\ttotal: 11.7s\tremaining: 1m 34s\n",
      "110:\tlearn: 0.4525243\ttotal: 11.8s\tremaining: 1m 34s\n",
      "111:\tlearn: 0.4523358\ttotal: 11.9s\tremaining: 1m 34s\n",
      "112:\tlearn: 0.4520335\ttotal: 12s\tremaining: 1m 34s\n",
      "113:\tlearn: 0.4513493\ttotal: 12.1s\tremaining: 1m 34s\n",
      "114:\tlearn: 0.4512054\ttotal: 12.2s\tremaining: 1m 33s\n",
      "115:\tlearn: 0.4503537\ttotal: 12.3s\tremaining: 1m 33s\n",
      "116:\tlearn: 0.4501570\ttotal: 12.4s\tremaining: 1m 33s\n",
      "117:\tlearn: 0.4499687\ttotal: 12.5s\tremaining: 1m 33s\n",
      "118:\tlearn: 0.4488263\ttotal: 12.6s\tremaining: 1m 33s\n",
      "119:\tlearn: 0.4486473\ttotal: 12.7s\tremaining: 1m 32s\n",
      "120:\tlearn: 0.4484801\ttotal: 12.8s\tremaining: 1m 32s\n",
      "121:\tlearn: 0.4481965\ttotal: 12.9s\tremaining: 1m 32s\n",
      "122:\tlearn: 0.4479570\ttotal: 13s\tremaining: 1m 32s\n",
      "123:\tlearn: 0.4467952\ttotal: 13.1s\tremaining: 1m 32s\n",
      "124:\tlearn: 0.4462824\ttotal: 13.1s\tremaining: 1m 32s\n",
      "125:\tlearn: 0.4460724\ttotal: 13.2s\tremaining: 1m 31s\n",
      "126:\tlearn: 0.4453020\ttotal: 13.3s\tremaining: 1m 31s\n",
      "127:\tlearn: 0.4448778\ttotal: 13.4s\tremaining: 1m 31s\n",
      "128:\tlearn: 0.4444739\ttotal: 13.5s\tremaining: 1m 31s\n",
      "129:\tlearn: 0.4433033\ttotal: 13.6s\tremaining: 1m 31s\n",
      "130:\tlearn: 0.4430301\ttotal: 13.7s\tremaining: 1m 30s\n",
      "131:\tlearn: 0.4427737\ttotal: 13.8s\tremaining: 1m 30s\n",
      "132:\tlearn: 0.4421617\ttotal: 13.9s\tremaining: 1m 30s\n",
      "133:\tlearn: 0.4418641\ttotal: 14.1s\tremaining: 1m 30s\n",
      "134:\tlearn: 0.4413962\ttotal: 14.2s\tremaining: 1m 31s\n",
      "135:\tlearn: 0.4410553\ttotal: 14.3s\tremaining: 1m 31s\n",
      "136:\tlearn: 0.4408201\ttotal: 14.4s\tremaining: 1m 30s\n",
      "137:\tlearn: 0.4405688\ttotal: 14.5s\tremaining: 1m 30s\n",
      "138:\tlearn: 0.4402357\ttotal: 14.6s\tremaining: 1m 30s\n",
      "139:\tlearn: 0.4400264\ttotal: 14.8s\tremaining: 1m 30s\n",
      "140:\tlearn: 0.4397387\ttotal: 14.8s\tremaining: 1m 30s\n",
      "141:\tlearn: 0.4386584\ttotal: 14.9s\tremaining: 1m 30s\n",
      "142:\tlearn: 0.4384675\ttotal: 15s\tremaining: 1m 29s\n",
      "143:\tlearn: 0.4380724\ttotal: 15.1s\tremaining: 1m 29s\n",
      "144:\tlearn: 0.4378690\ttotal: 15.2s\tremaining: 1m 29s\n",
      "145:\tlearn: 0.4375842\ttotal: 15.3s\tremaining: 1m 29s\n",
      "146:\tlearn: 0.4374294\ttotal: 15.4s\tremaining: 1m 29s\n",
      "147:\tlearn: 0.4370540\ttotal: 15.6s\tremaining: 1m 29s\n",
      "148:\tlearn: 0.4368720\ttotal: 15.7s\tremaining: 1m 29s\n",
      "149:\tlearn: 0.4365879\ttotal: 15.7s\tremaining: 1m 29s\n",
      "150:\tlearn: 0.4362386\ttotal: 15.8s\tremaining: 1m 28s\n",
      "151:\tlearn: 0.4358993\ttotal: 15.9s\tremaining: 1m 28s\n",
      "152:\tlearn: 0.4355314\ttotal: 16.1s\tremaining: 1m 28s\n",
      "153:\tlearn: 0.4352337\ttotal: 16.2s\tremaining: 1m 28s\n",
      "154:\tlearn: 0.4346119\ttotal: 16.2s\tremaining: 1m 28s\n",
      "155:\tlearn: 0.4339041\ttotal: 16.3s\tremaining: 1m 28s\n",
      "156:\tlearn: 0.4332188\ttotal: 16.5s\tremaining: 1m 28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157:\tlearn: 0.4321440\ttotal: 16.6s\tremaining: 1m 28s\n",
      "158:\tlearn: 0.4318991\ttotal: 16.7s\tremaining: 1m 28s\n",
      "159:\tlearn: 0.4311032\ttotal: 16.8s\tremaining: 1m 28s\n",
      "160:\tlearn: 0.4305187\ttotal: 16.9s\tremaining: 1m 28s\n",
      "161:\tlearn: 0.4303383\ttotal: 17s\tremaining: 1m 28s\n",
      "162:\tlearn: 0.4295332\ttotal: 17.1s\tremaining: 1m 27s\n",
      "163:\tlearn: 0.4292957\ttotal: 17.2s\tremaining: 1m 27s\n",
      "164:\tlearn: 0.4287811\ttotal: 17.4s\tremaining: 1m 27s\n",
      "165:\tlearn: 0.4284887\ttotal: 17.5s\tremaining: 1m 28s\n",
      "166:\tlearn: 0.4282356\ttotal: 17.6s\tremaining: 1m 27s\n",
      "167:\tlearn: 0.4280464\ttotal: 17.8s\tremaining: 1m 27s\n",
      "168:\tlearn: 0.4278839\ttotal: 17.8s\tremaining: 1m 27s\n",
      "169:\tlearn: 0.4274566\ttotal: 17.9s\tremaining: 1m 27s\n",
      "170:\tlearn: 0.4270726\ttotal: 18.1s\tremaining: 1m 27s\n",
      "171:\tlearn: 0.4268577\ttotal: 18.3s\tremaining: 1m 27s\n",
      "172:\tlearn: 0.4266816\ttotal: 18.4s\tremaining: 1m 27s\n",
      "173:\tlearn: 0.4265225\ttotal: 18.5s\tremaining: 1m 27s\n",
      "174:\tlearn: 0.4262924\ttotal: 18.6s\tremaining: 1m 27s\n",
      "175:\tlearn: 0.4255487\ttotal: 18.8s\tremaining: 1m 27s\n",
      "176:\tlearn: 0.4253497\ttotal: 18.9s\tremaining: 1m 27s\n",
      "177:\tlearn: 0.4251569\ttotal: 19s\tremaining: 1m 27s\n",
      "178:\tlearn: 0.4249232\ttotal: 19.1s\tremaining: 1m 27s\n",
      "179:\tlearn: 0.4244802\ttotal: 19.2s\tremaining: 1m 27s\n",
      "180:\tlearn: 0.4241743\ttotal: 19.3s\tremaining: 1m 27s\n",
      "181:\tlearn: 0.4235888\ttotal: 19.4s\tremaining: 1m 27s\n",
      "182:\tlearn: 0.4229499\ttotal: 19.5s\tremaining: 1m 27s\n",
      "183:\tlearn: 0.4227870\ttotal: 19.6s\tremaining: 1m 27s\n",
      "184:\tlearn: 0.4226322\ttotal: 19.8s\tremaining: 1m 27s\n",
      "185:\tlearn: 0.4222307\ttotal: 19.9s\tremaining: 1m 26s\n",
      "186:\tlearn: 0.4220740\ttotal: 20s\tremaining: 1m 26s\n",
      "187:\tlearn: 0.4218415\ttotal: 20.1s\tremaining: 1m 26s\n",
      "188:\tlearn: 0.4211083\ttotal: 20.2s\tremaining: 1m 26s\n",
      "189:\tlearn: 0.4207507\ttotal: 20.3s\tremaining: 1m 26s\n",
      "190:\tlearn: 0.4203266\ttotal: 20.4s\tremaining: 1m 26s\n",
      "191:\tlearn: 0.4200067\ttotal: 20.5s\tremaining: 1m 26s\n",
      "192:\tlearn: 0.4193149\ttotal: 20.6s\tremaining: 1m 26s\n",
      "193:\tlearn: 0.4189718\ttotal: 20.8s\tremaining: 1m 26s\n",
      "194:\tlearn: 0.4187121\ttotal: 20.9s\tremaining: 1m 26s\n",
      "195:\tlearn: 0.4184778\ttotal: 21s\tremaining: 1m 26s\n",
      "196:\tlearn: 0.4182543\ttotal: 21.1s\tremaining: 1m 25s\n",
      "197:\tlearn: 0.4177585\ttotal: 21.2s\tremaining: 1m 25s\n",
      "198:\tlearn: 0.4176081\ttotal: 21.3s\tremaining: 1m 25s\n",
      "199:\tlearn: 0.4171874\ttotal: 21.4s\tremaining: 1m 25s\n",
      "200:\tlearn: 0.4170303\ttotal: 21.5s\tremaining: 1m 25s\n",
      "201:\tlearn: 0.4166689\ttotal: 21.6s\tremaining: 1m 25s\n",
      "202:\tlearn: 0.4165424\ttotal: 21.6s\tremaining: 1m 25s\n",
      "203:\tlearn: 0.4163317\ttotal: 21.8s\tremaining: 1m 24s\n",
      "204:\tlearn: 0.4157038\ttotal: 21.9s\tremaining: 1m 25s\n",
      "205:\tlearn: 0.4155485\ttotal: 22s\tremaining: 1m 24s\n",
      "206:\tlearn: 0.4152572\ttotal: 22.2s\tremaining: 1m 24s\n",
      "207:\tlearn: 0.4142926\ttotal: 22.3s\tremaining: 1m 24s\n",
      "208:\tlearn: 0.4141238\ttotal: 22.4s\tremaining: 1m 24s\n",
      "209:\tlearn: 0.4139492\ttotal: 22.5s\tremaining: 1m 24s\n",
      "210:\tlearn: 0.4135335\ttotal: 22.6s\tremaining: 1m 24s\n",
      "211:\tlearn: 0.4133509\ttotal: 22.7s\tremaining: 1m 24s\n",
      "212:\tlearn: 0.4132152\ttotal: 22.8s\tremaining: 1m 24s\n",
      "213:\tlearn: 0.4130578\ttotal: 22.9s\tremaining: 1m 24s\n",
      "214:\tlearn: 0.4125041\ttotal: 23s\tremaining: 1m 24s\n",
      "215:\tlearn: 0.4120743\ttotal: 23.1s\tremaining: 1m 24s\n",
      "216:\tlearn: 0.4119395\ttotal: 23.2s\tremaining: 1m 23s\n",
      "217:\tlearn: 0.4116455\ttotal: 23.3s\tremaining: 1m 23s\n",
      "218:\tlearn: 0.4113514\ttotal: 23.5s\tremaining: 1m 23s\n",
      "219:\tlearn: 0.4112109\ttotal: 23.6s\tremaining: 1m 23s\n",
      "220:\tlearn: 0.4110479\ttotal: 23.7s\tremaining: 1m 23s\n",
      "221:\tlearn: 0.4109090\ttotal: 23.8s\tremaining: 1m 23s\n",
      "222:\tlearn: 0.4106159\ttotal: 23.9s\tremaining: 1m 23s\n",
      "223:\tlearn: 0.4103892\ttotal: 24s\tremaining: 1m 23s\n",
      "224:\tlearn: 0.4098507\ttotal: 24.1s\tremaining: 1m 22s\n",
      "225:\tlearn: 0.4097169\ttotal: 24.1s\tremaining: 1m 22s\n",
      "226:\tlearn: 0.4091296\ttotal: 24.3s\tremaining: 1m 22s\n",
      "227:\tlearn: 0.4088595\ttotal: 24.4s\tremaining: 1m 22s\n",
      "228:\tlearn: 0.4086090\ttotal: 24.5s\tremaining: 1m 22s\n",
      "229:\tlearn: 0.4084233\ttotal: 24.6s\tremaining: 1m 22s\n",
      "230:\tlearn: 0.4082713\ttotal: 24.7s\tremaining: 1m 22s\n",
      "231:\tlearn: 0.4075366\ttotal: 24.8s\tremaining: 1m 22s\n",
      "232:\tlearn: 0.4074165\ttotal: 24.9s\tremaining: 1m 22s\n",
      "233:\tlearn: 0.4070890\ttotal: 25.1s\tremaining: 1m 22s\n",
      "234:\tlearn: 0.4068569\ttotal: 25.2s\tremaining: 1m 22s\n",
      "235:\tlearn: 0.4064070\ttotal: 25.4s\tremaining: 1m 22s\n",
      "236:\tlearn: 0.4061897\ttotal: 25.5s\tremaining: 1m 21s\n",
      "237:\tlearn: 0.4060238\ttotal: 25.6s\tremaining: 1m 21s\n",
      "238:\tlearn: 0.4058500\ttotal: 25.7s\tremaining: 1m 21s\n",
      "239:\tlearn: 0.4055704\ttotal: 25.8s\tremaining: 1m 21s\n",
      "240:\tlearn: 0.4054475\ttotal: 25.9s\tremaining: 1m 21s\n",
      "241:\tlearn: 0.4053410\ttotal: 26s\tremaining: 1m 21s\n",
      "242:\tlearn: 0.4049206\ttotal: 26.1s\tremaining: 1m 21s\n",
      "243:\tlearn: 0.4045992\ttotal: 26.2s\tremaining: 1m 21s\n",
      "244:\tlearn: 0.4044592\ttotal: 26.3s\tremaining: 1m 20s\n",
      "245:\tlearn: 0.4038006\ttotal: 26.4s\tremaining: 1m 20s\n",
      "246:\tlearn: 0.4033625\ttotal: 26.5s\tremaining: 1m 20s\n",
      "247:\tlearn: 0.4031904\ttotal: 26.6s\tremaining: 1m 20s\n",
      "248:\tlearn: 0.4028788\ttotal: 26.7s\tremaining: 1m 20s\n",
      "249:\tlearn: 0.4026963\ttotal: 26.8s\tremaining: 1m 20s\n",
      "250:\tlearn: 0.4025584\ttotal: 26.9s\tremaining: 1m 20s\n",
      "251:\tlearn: 0.4019306\ttotal: 27s\tremaining: 1m 20s\n",
      "252:\tlearn: 0.4014357\ttotal: 27.1s\tremaining: 1m 20s\n",
      "253:\tlearn: 0.4012225\ttotal: 27.3s\tremaining: 1m 20s\n",
      "254:\tlearn: 0.4008239\ttotal: 27.4s\tremaining: 1m 20s\n",
      "255:\tlearn: 0.4002771\ttotal: 27.5s\tremaining: 1m 20s\n",
      "256:\tlearn: 0.4001649\ttotal: 27.6s\tremaining: 1m 19s\n",
      "257:\tlearn: 0.4000098\ttotal: 27.7s\tremaining: 1m 19s\n",
      "258:\tlearn: 0.3997876\ttotal: 27.8s\tremaining: 1m 19s\n",
      "259:\tlearn: 0.3993765\ttotal: 27.9s\tremaining: 1m 19s\n",
      "260:\tlearn: 0.3992344\ttotal: 28s\tremaining: 1m 19s\n",
      "261:\tlearn: 0.3991020\ttotal: 28.1s\tremaining: 1m 19s\n",
      "262:\tlearn: 0.3983946\ttotal: 28.2s\tremaining: 1m 19s\n",
      "263:\tlearn: 0.3979514\ttotal: 28.3s\tremaining: 1m 18s\n",
      "264:\tlearn: 0.3975559\ttotal: 28.5s\tremaining: 1m 18s\n",
      "265:\tlearn: 0.3974412\ttotal: 28.5s\tremaining: 1m 18s\n",
      "266:\tlearn: 0.3973551\ttotal: 28.6s\tremaining: 1m 18s\n",
      "267:\tlearn: 0.3969805\ttotal: 28.7s\tremaining: 1m 18s\n",
      "268:\tlearn: 0.3967932\ttotal: 28.9s\tremaining: 1m 18s\n",
      "269:\tlearn: 0.3966123\ttotal: 29s\tremaining: 1m 18s\n",
      "270:\tlearn: 0.3964154\ttotal: 29.1s\tremaining: 1m 18s\n",
      "271:\tlearn: 0.3962984\ttotal: 29.2s\tremaining: 1m 18s\n",
      "272:\tlearn: 0.3959593\ttotal: 29.3s\tremaining: 1m 18s\n",
      "273:\tlearn: 0.3956505\ttotal: 29.4s\tremaining: 1m 18s\n",
      "274:\tlearn: 0.3954183\ttotal: 29.6s\tremaining: 1m 17s\n",
      "275:\tlearn: 0.3951614\ttotal: 29.7s\tremaining: 1m 17s\n",
      "276:\tlearn: 0.3950291\ttotal: 29.8s\tremaining: 1m 17s\n",
      "277:\tlearn: 0.3948164\ttotal: 30s\tremaining: 1m 17s\n",
      "278:\tlearn: 0.3946148\ttotal: 30s\tremaining: 1m 17s\n",
      "279:\tlearn: 0.3945231\ttotal: 30.1s\tremaining: 1m 17s\n",
      "280:\tlearn: 0.3943546\ttotal: 30.2s\tremaining: 1m 17s\n",
      "281:\tlearn: 0.3939654\ttotal: 30.3s\tremaining: 1m 17s\n",
      "282:\tlearn: 0.3938253\ttotal: 30.4s\tremaining: 1m 17s\n",
      "283:\tlearn: 0.3933027\ttotal: 30.5s\tremaining: 1m 16s\n",
      "284:\tlearn: 0.3932033\ttotal: 30.6s\tremaining: 1m 16s\n",
      "285:\tlearn: 0.3929697\ttotal: 30.7s\tremaining: 1m 16s\n",
      "286:\tlearn: 0.3928261\ttotal: 30.8s\tremaining: 1m 16s\n",
      "287:\tlearn: 0.3926406\ttotal: 30.9s\tremaining: 1m 16s\n",
      "288:\tlearn: 0.3925177\ttotal: 31s\tremaining: 1m 16s\n",
      "289:\tlearn: 0.3924130\ttotal: 31.1s\tremaining: 1m 16s\n",
      "290:\tlearn: 0.3922438\ttotal: 31.2s\tremaining: 1m 16s\n",
      "291:\tlearn: 0.3921069\ttotal: 31.3s\tremaining: 1m 15s\n",
      "292:\tlearn: 0.3920115\ttotal: 31.4s\tremaining: 1m 15s\n",
      "293:\tlearn: 0.3912286\ttotal: 31.5s\tremaining: 1m 15s\n",
      "294:\tlearn: 0.3910234\ttotal: 31.6s\tremaining: 1m 15s\n",
      "295:\tlearn: 0.3908029\ttotal: 31.7s\tremaining: 1m 15s\n",
      "296:\tlearn: 0.3907092\ttotal: 31.8s\tremaining: 1m 15s\n",
      "297:\tlearn: 0.3905013\ttotal: 31.9s\tremaining: 1m 15s\n",
      "298:\tlearn: 0.3900008\ttotal: 32.1s\tremaining: 1m 15s\n",
      "299:\tlearn: 0.3898860\ttotal: 32.2s\tremaining: 1m 15s\n",
      "300:\tlearn: 0.3897868\ttotal: 32.3s\tremaining: 1m 14s\n",
      "301:\tlearn: 0.3896957\ttotal: 32.4s\tremaining: 1m 14s\n",
      "302:\tlearn: 0.3895727\ttotal: 32.5s\tremaining: 1m 14s\n",
      "303:\tlearn: 0.3891673\ttotal: 32.6s\tremaining: 1m 14s\n",
      "304:\tlearn: 0.3889923\ttotal: 32.7s\tremaining: 1m 14s\n",
      "305:\tlearn: 0.3887784\ttotal: 32.8s\tremaining: 1m 14s\n",
      "306:\tlearn: 0.3885586\ttotal: 32.9s\tremaining: 1m 14s\n",
      "307:\tlearn: 0.3884517\ttotal: 33s\tremaining: 1m 14s\n",
      "308:\tlearn: 0.3881467\ttotal: 33.1s\tremaining: 1m 14s\n",
      "309:\tlearn: 0.3880171\ttotal: 33.2s\tremaining: 1m 13s\n",
      "310:\tlearn: 0.3878989\ttotal: 33.3s\tremaining: 1m 13s\n",
      "311:\tlearn: 0.3876846\ttotal: 33.4s\tremaining: 1m 13s\n",
      "312:\tlearn: 0.3876110\ttotal: 33.5s\tremaining: 1m 13s\n",
      "313:\tlearn: 0.3874642\ttotal: 33.6s\tremaining: 1m 13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314:\tlearn: 0.3873285\ttotal: 33.7s\tremaining: 1m 13s\n",
      "315:\tlearn: 0.3871595\ttotal: 33.8s\tremaining: 1m 13s\n",
      "316:\tlearn: 0.3865046\ttotal: 34s\tremaining: 1m 13s\n",
      "317:\tlearn: 0.3863225\ttotal: 34.1s\tremaining: 1m 13s\n",
      "318:\tlearn: 0.3859425\ttotal: 34.2s\tremaining: 1m 12s\n",
      "319:\tlearn: 0.3858763\ttotal: 34.3s\tremaining: 1m 12s\n",
      "320:\tlearn: 0.3857528\ttotal: 34.4s\tremaining: 1m 12s\n",
      "321:\tlearn: 0.3856309\ttotal: 34.5s\tremaining: 1m 12s\n",
      "322:\tlearn: 0.3853929\ttotal: 34.6s\tremaining: 1m 12s\n",
      "323:\tlearn: 0.3852002\ttotal: 34.7s\tremaining: 1m 12s\n",
      "324:\tlearn: 0.3850875\ttotal: 34.8s\tremaining: 1m 12s\n",
      "325:\tlearn: 0.3850164\ttotal: 34.8s\tremaining: 1m 12s\n",
      "326:\tlearn: 0.3846971\ttotal: 35s\tremaining: 1m 12s\n",
      "327:\tlearn: 0.3845840\ttotal: 35.1s\tremaining: 1m 11s\n",
      "328:\tlearn: 0.3840185\ttotal: 35.2s\tremaining: 1m 11s\n",
      "329:\tlearn: 0.3839236\ttotal: 35.2s\tremaining: 1m 11s\n",
      "330:\tlearn: 0.3838254\ttotal: 35.3s\tremaining: 1m 11s\n",
      "331:\tlearn: 0.3837180\ttotal: 35.4s\tremaining: 1m 11s\n",
      "332:\tlearn: 0.3835693\ttotal: 35.5s\tremaining: 1m 11s\n",
      "333:\tlearn: 0.3831114\ttotal: 35.6s\tremaining: 1m 11s\n",
      "334:\tlearn: 0.3829717\ttotal: 35.7s\tremaining: 1m 10s\n",
      "335:\tlearn: 0.3825585\ttotal: 35.8s\tremaining: 1m 10s\n",
      "336:\tlearn: 0.3822718\ttotal: 35.9s\tremaining: 1m 10s\n",
      "337:\tlearn: 0.3819373\ttotal: 36s\tremaining: 1m 10s\n",
      "338:\tlearn: 0.3817978\ttotal: 36.1s\tremaining: 1m 10s\n",
      "339:\tlearn: 0.3816404\ttotal: 36.3s\tremaining: 1m 10s\n",
      "340:\tlearn: 0.3812481\ttotal: 36.4s\tremaining: 1m 10s\n",
      "341:\tlearn: 0.3811038\ttotal: 36.5s\tremaining: 1m 10s\n",
      "342:\tlearn: 0.3809339\ttotal: 36.6s\tremaining: 1m 10s\n",
      "343:\tlearn: 0.3808547\ttotal: 36.7s\tremaining: 1m 10s\n",
      "344:\tlearn: 0.3807854\ttotal: 36.8s\tremaining: 1m 9s\n",
      "345:\tlearn: 0.3806091\ttotal: 36.9s\tremaining: 1m 9s\n",
      "346:\tlearn: 0.3804505\ttotal: 37.1s\tremaining: 1m 9s\n",
      "347:\tlearn: 0.3803351\ttotal: 37.2s\tremaining: 1m 9s\n",
      "348:\tlearn: 0.3802644\ttotal: 37.3s\tremaining: 1m 9s\n",
      "349:\tlearn: 0.3801709\ttotal: 37.3s\tremaining: 1m 9s\n",
      "350:\tlearn: 0.3800272\ttotal: 37.4s\tremaining: 1m 9s\n",
      "351:\tlearn: 0.3799276\ttotal: 37.5s\tremaining: 1m 9s\n",
      "352:\tlearn: 0.3798524\ttotal: 37.6s\tremaining: 1m 8s\n",
      "353:\tlearn: 0.3795676\ttotal: 37.7s\tremaining: 1m 8s\n",
      "354:\tlearn: 0.3793075\ttotal: 37.8s\tremaining: 1m 8s\n",
      "355:\tlearn: 0.3791279\ttotal: 38s\tremaining: 1m 8s\n",
      "356:\tlearn: 0.3789541\ttotal: 38.1s\tremaining: 1m 8s\n",
      "357:\tlearn: 0.3788880\ttotal: 38.2s\tremaining: 1m 8s\n",
      "358:\tlearn: 0.3784042\ttotal: 38.3s\tremaining: 1m 8s\n",
      "359:\tlearn: 0.3782720\ttotal: 38.4s\tremaining: 1m 8s\n",
      "360:\tlearn: 0.3776582\ttotal: 38.5s\tremaining: 1m 8s\n",
      "361:\tlearn: 0.3771592\ttotal: 38.6s\tremaining: 1m 8s\n",
      "362:\tlearn: 0.3770023\ttotal: 38.8s\tremaining: 1m 8s\n",
      "363:\tlearn: 0.3769054\ttotal: 38.9s\tremaining: 1m 7s\n",
      "364:\tlearn: 0.3764382\ttotal: 39s\tremaining: 1m 7s\n",
      "365:\tlearn: 0.3761161\ttotal: 39.1s\tremaining: 1m 7s\n",
      "366:\tlearn: 0.3760384\ttotal: 39.1s\tremaining: 1m 7s\n",
      "367:\tlearn: 0.3756756\ttotal: 39.2s\tremaining: 1m 7s\n",
      "368:\tlearn: 0.3755490\ttotal: 39.3s\tremaining: 1m 7s\n",
      "369:\tlearn: 0.3753838\ttotal: 39.4s\tremaining: 1m 7s\n",
      "370:\tlearn: 0.3752697\ttotal: 39.6s\tremaining: 1m 7s\n",
      "371:\tlearn: 0.3751791\ttotal: 39.7s\tremaining: 1m 6s\n",
      "372:\tlearn: 0.3751041\ttotal: 39.7s\tremaining: 1m 6s\n",
      "373:\tlearn: 0.3749898\ttotal: 39.9s\tremaining: 1m 6s\n",
      "374:\tlearn: 0.3747459\ttotal: 40s\tremaining: 1m 6s\n",
      "375:\tlearn: 0.3746498\ttotal: 40.1s\tremaining: 1m 6s\n",
      "376:\tlearn: 0.3744618\ttotal: 40.2s\tremaining: 1m 6s\n",
      "377:\tlearn: 0.3741813\ttotal: 40.3s\tremaining: 1m 6s\n",
      "378:\tlearn: 0.3740907\ttotal: 40.4s\tremaining: 1m 6s\n",
      "379:\tlearn: 0.3739937\ttotal: 40.5s\tremaining: 1m 6s\n",
      "380:\tlearn: 0.3739184\ttotal: 40.6s\tremaining: 1m 5s\n",
      "381:\tlearn: 0.3738431\ttotal: 40.7s\tremaining: 1m 5s\n",
      "382:\tlearn: 0.3735027\ttotal: 40.8s\tremaining: 1m 5s\n",
      "383:\tlearn: 0.3734041\ttotal: 40.9s\tremaining: 1m 5s\n",
      "384:\tlearn: 0.3732936\ttotal: 41s\tremaining: 1m 5s\n",
      "385:\tlearn: 0.3730319\ttotal: 41.1s\tremaining: 1m 5s\n",
      "386:\tlearn: 0.3729012\ttotal: 41.2s\tremaining: 1m 5s\n",
      "387:\tlearn: 0.3727803\ttotal: 41.3s\tremaining: 1m 5s\n",
      "388:\tlearn: 0.3726486\ttotal: 41.4s\tremaining: 1m 5s\n",
      "389:\tlearn: 0.3725709\ttotal: 41.5s\tremaining: 1m 4s\n",
      "390:\tlearn: 0.3724769\ttotal: 41.6s\tremaining: 1m 4s\n",
      "391:\tlearn: 0.3720739\ttotal: 41.7s\tremaining: 1m 4s\n",
      "392:\tlearn: 0.3719453\ttotal: 41.9s\tremaining: 1m 4s\n",
      "393:\tlearn: 0.3716815\ttotal: 42s\tremaining: 1m 4s\n",
      "394:\tlearn: 0.3716224\ttotal: 42.1s\tremaining: 1m 4s\n",
      "395:\tlearn: 0.3715383\ttotal: 42.2s\tremaining: 1m 4s\n",
      "396:\tlearn: 0.3714239\ttotal: 42.3s\tremaining: 1m 4s\n",
      "397:\tlearn: 0.3713688\ttotal: 42.3s\tremaining: 1m 4s\n",
      "398:\tlearn: 0.3710398\ttotal: 42.5s\tremaining: 1m 3s\n",
      "399:\tlearn: 0.3709601\ttotal: 42.5s\tremaining: 1m 3s\n",
      "400:\tlearn: 0.3706867\ttotal: 42.6s\tremaining: 1m 3s\n",
      "401:\tlearn: 0.3704309\ttotal: 42.8s\tremaining: 1m 3s\n",
      "402:\tlearn: 0.3702972\ttotal: 42.8s\tremaining: 1m 3s\n",
      "403:\tlearn: 0.3701236\ttotal: 43s\tremaining: 1m 3s\n",
      "404:\tlearn: 0.3700496\ttotal: 43s\tremaining: 1m 3s\n",
      "405:\tlearn: 0.3698966\ttotal: 43.2s\tremaining: 1m 3s\n",
      "406:\tlearn: 0.3698169\ttotal: 43.3s\tremaining: 1m 3s\n",
      "407:\tlearn: 0.3696115\ttotal: 43.4s\tremaining: 1m 2s\n",
      "408:\tlearn: 0.3694827\ttotal: 43.5s\tremaining: 1m 2s\n",
      "409:\tlearn: 0.3694214\ttotal: 43.6s\tremaining: 1m 2s\n",
      "410:\tlearn: 0.3693015\ttotal: 43.7s\tremaining: 1m 2s\n",
      "411:\tlearn: 0.3691923\ttotal: 43.8s\tremaining: 1m 2s\n",
      "412:\tlearn: 0.3691232\ttotal: 43.9s\tremaining: 1m 2s\n",
      "413:\tlearn: 0.3689583\ttotal: 44s\tremaining: 1m 2s\n",
      "414:\tlearn: 0.3686822\ttotal: 44.1s\tremaining: 1m 2s\n",
      "415:\tlearn: 0.3684515\ttotal: 44.2s\tremaining: 1m 2s\n",
      "416:\tlearn: 0.3681359\ttotal: 44.4s\tremaining: 1m 2s\n",
      "417:\tlearn: 0.3680328\ttotal: 44.5s\tremaining: 1m 1s\n",
      "418:\tlearn: 0.3679033\ttotal: 44.6s\tremaining: 1m 1s\n",
      "419:\tlearn: 0.3678300\ttotal: 44.6s\tremaining: 1m 1s\n",
      "420:\tlearn: 0.3676004\ttotal: 44.7s\tremaining: 1m 1s\n",
      "421:\tlearn: 0.3675123\ttotal: 44.8s\tremaining: 1m 1s\n",
      "422:\tlearn: 0.3673827\ttotal: 44.9s\tremaining: 1m 1s\n",
      "423:\tlearn: 0.3672815\ttotal: 45s\tremaining: 1m 1s\n",
      "424:\tlearn: 0.3672036\ttotal: 45.1s\tremaining: 1m 1s\n",
      "425:\tlearn: 0.3671113\ttotal: 45.2s\tremaining: 1m\n",
      "426:\tlearn: 0.3668707\ttotal: 45.3s\tremaining: 1m\n",
      "427:\tlearn: 0.3668307\ttotal: 45.4s\tremaining: 1m\n",
      "428:\tlearn: 0.3667386\ttotal: 45.4s\tremaining: 1m\n",
      "429:\tlearn: 0.3662175\ttotal: 45.5s\tremaining: 1m\n",
      "430:\tlearn: 0.3661825\ttotal: 45.6s\tremaining: 1m\n",
      "431:\tlearn: 0.3659916\ttotal: 45.7s\tremaining: 1m\n",
      "432:\tlearn: 0.3659296\ttotal: 45.8s\tremaining: 60s\n",
      "433:\tlearn: 0.3656528\ttotal: 45.9s\tremaining: 59.9s\n",
      "434:\tlearn: 0.3655466\ttotal: 46.1s\tremaining: 59.8s\n",
      "435:\tlearn: 0.3651724\ttotal: 46.2s\tremaining: 59.7s\n",
      "436:\tlearn: 0.3650952\ttotal: 46.3s\tremaining: 59.6s\n",
      "437:\tlearn: 0.3650048\ttotal: 46.4s\tremaining: 59.5s\n",
      "438:\tlearn: 0.3649239\ttotal: 46.5s\tremaining: 59.4s\n",
      "439:\tlearn: 0.3647886\ttotal: 46.6s\tremaining: 59.3s\n",
      "440:\tlearn: 0.3647380\ttotal: 46.6s\tremaining: 59.1s\n",
      "441:\tlearn: 0.3646295\ttotal: 46.8s\tremaining: 59s\n",
      "442:\tlearn: 0.3644750\ttotal: 46.9s\tremaining: 59s\n",
      "443:\tlearn: 0.3643513\ttotal: 47s\tremaining: 58.9s\n",
      "444:\tlearn: 0.3642956\ttotal: 47.1s\tremaining: 58.8s\n",
      "445:\tlearn: 0.3641326\ttotal: 47.3s\tremaining: 58.7s\n",
      "446:\tlearn: 0.3637178\ttotal: 47.4s\tremaining: 58.6s\n",
      "447:\tlearn: 0.3636385\ttotal: 47.5s\tremaining: 58.5s\n",
      "448:\tlearn: 0.3635578\ttotal: 47.5s\tremaining: 58.3s\n",
      "449:\tlearn: 0.3634333\ttotal: 47.6s\tremaining: 58.2s\n",
      "450:\tlearn: 0.3633693\ttotal: 47.7s\tremaining: 58.1s\n",
      "451:\tlearn: 0.3632049\ttotal: 47.8s\tremaining: 58s\n",
      "452:\tlearn: 0.3631068\ttotal: 47.9s\tremaining: 57.9s\n",
      "453:\tlearn: 0.3630353\ttotal: 48s\tremaining: 57.7s\n",
      "454:\tlearn: 0.3629363\ttotal: 48.1s\tremaining: 57.6s\n",
      "455:\tlearn: 0.3628103\ttotal: 48.2s\tremaining: 57.5s\n",
      "456:\tlearn: 0.3627216\ttotal: 48.3s\tremaining: 57.4s\n",
      "457:\tlearn: 0.3626204\ttotal: 48.4s\tremaining: 57.3s\n",
      "458:\tlearn: 0.3624567\ttotal: 48.5s\tremaining: 57.2s\n",
      "459:\tlearn: 0.3623693\ttotal: 48.6s\tremaining: 57s\n",
      "460:\tlearn: 0.3621157\ttotal: 48.7s\tremaining: 56.9s\n",
      "461:\tlearn: 0.3619347\ttotal: 48.8s\tremaining: 56.9s\n",
      "462:\tlearn: 0.3618642\ttotal: 48.9s\tremaining: 56.7s\n",
      "463:\tlearn: 0.3618067\ttotal: 49s\tremaining: 56.6s\n",
      "464:\tlearn: 0.3617315\ttotal: 49.1s\tremaining: 56.5s\n",
      "465:\tlearn: 0.3616196\ttotal: 49.2s\tremaining: 56.4s\n",
      "466:\tlearn: 0.3615336\ttotal: 49.3s\tremaining: 56.3s\n",
      "467:\tlearn: 0.3613872\ttotal: 49.4s\tremaining: 56.2s\n",
      "468:\tlearn: 0.3613355\ttotal: 49.5s\tremaining: 56s\n",
      "469:\tlearn: 0.3612192\ttotal: 49.6s\tremaining: 56s\n",
      "470:\tlearn: 0.3610728\ttotal: 49.8s\tremaining: 55.9s\n",
      "471:\tlearn: 0.3609807\ttotal: 49.9s\tremaining: 55.8s\n",
      "472:\tlearn: 0.3608426\ttotal: 50s\tremaining: 55.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473:\tlearn: 0.3607653\ttotal: 50.1s\tremaining: 55.6s\n",
      "474:\tlearn: 0.3606709\ttotal: 50.2s\tremaining: 55.5s\n",
      "475:\tlearn: 0.3605053\ttotal: 50.4s\tremaining: 55.5s\n",
      "476:\tlearn: 0.3603575\ttotal: 50.5s\tremaining: 55.3s\n",
      "477:\tlearn: 0.3601808\ttotal: 50.6s\tremaining: 55.3s\n",
      "478:\tlearn: 0.3599198\ttotal: 50.7s\tremaining: 55.2s\n",
      "479:\tlearn: 0.3597858\ttotal: 50.8s\tremaining: 55s\n",
      "480:\tlearn: 0.3596581\ttotal: 51s\tremaining: 55s\n",
      "481:\tlearn: 0.3594426\ttotal: 51.1s\tremaining: 54.9s\n",
      "482:\tlearn: 0.3593531\ttotal: 51.2s\tremaining: 54.8s\n",
      "483:\tlearn: 0.3592801\ttotal: 51.2s\tremaining: 54.6s\n",
      "484:\tlearn: 0.3591923\ttotal: 51.3s\tremaining: 54.5s\n",
      "485:\tlearn: 0.3591287\ttotal: 51.4s\tremaining: 54.4s\n",
      "486:\tlearn: 0.3589143\ttotal: 51.6s\tremaining: 54.3s\n",
      "487:\tlearn: 0.3587460\ttotal: 51.7s\tremaining: 54.2s\n",
      "488:\tlearn: 0.3586785\ttotal: 51.8s\tremaining: 54.1s\n",
      "489:\tlearn: 0.3585609\ttotal: 51.9s\tremaining: 54s\n",
      "490:\tlearn: 0.3584633\ttotal: 52s\tremaining: 53.9s\n",
      "491:\tlearn: 0.3583221\ttotal: 52.1s\tremaining: 53.8s\n",
      "492:\tlearn: 0.3582287\ttotal: 52.3s\tremaining: 53.7s\n",
      "493:\tlearn: 0.3581332\ttotal: 52.4s\tremaining: 53.6s\n",
      "494:\tlearn: 0.3580514\ttotal: 52.4s\tremaining: 53.5s\n",
      "495:\tlearn: 0.3579084\ttotal: 52.5s\tremaining: 53.4s\n",
      "496:\tlearn: 0.3578156\ttotal: 52.7s\tremaining: 53.3s\n",
      "497:\tlearn: 0.3577456\ttotal: 52.8s\tremaining: 53.2s\n",
      "498:\tlearn: 0.3576667\ttotal: 52.9s\tremaining: 53.1s\n",
      "499:\tlearn: 0.3576037\ttotal: 53s\tremaining: 53s\n",
      "500:\tlearn: 0.3575048\ttotal: 53.1s\tremaining: 52.8s\n",
      "501:\tlearn: 0.3574188\ttotal: 53.2s\tremaining: 52.7s\n",
      "502:\tlearn: 0.3572879\ttotal: 53.3s\tremaining: 52.7s\n",
      "503:\tlearn: 0.3572025\ttotal: 53.4s\tremaining: 52.5s\n",
      "504:\tlearn: 0.3571341\ttotal: 53.5s\tremaining: 52.4s\n",
      "505:\tlearn: 0.3569391\ttotal: 53.6s\tremaining: 52.3s\n",
      "506:\tlearn: 0.3568682\ttotal: 53.6s\tremaining: 52.2s\n",
      "507:\tlearn: 0.3567445\ttotal: 53.8s\tremaining: 52.1s\n",
      "508:\tlearn: 0.3566947\ttotal: 53.8s\tremaining: 51.9s\n",
      "509:\tlearn: 0.3566297\ttotal: 53.9s\tremaining: 51.8s\n",
      "510:\tlearn: 0.3564704\ttotal: 54.1s\tremaining: 51.7s\n",
      "511:\tlearn: 0.3562943\ttotal: 54.2s\tremaining: 51.7s\n",
      "512:\tlearn: 0.3559626\ttotal: 54.3s\tremaining: 51.5s\n",
      "513:\tlearn: 0.3558982\ttotal: 54.4s\tremaining: 51.4s\n",
      "514:\tlearn: 0.3557782\ttotal: 54.5s\tremaining: 51.3s\n",
      "515:\tlearn: 0.3557251\ttotal: 54.6s\tremaining: 51.2s\n",
      "516:\tlearn: 0.3556223\ttotal: 54.7s\tremaining: 51.1s\n",
      "517:\tlearn: 0.3555606\ttotal: 54.8s\tremaining: 51s\n",
      "518:\tlearn: 0.3554889\ttotal: 54.9s\tremaining: 50.9s\n",
      "519:\tlearn: 0.3551144\ttotal: 55s\tremaining: 50.8s\n",
      "520:\tlearn: 0.3550069\ttotal: 55.1s\tremaining: 50.6s\n",
      "521:\tlearn: 0.3546323\ttotal: 55.2s\tremaining: 50.5s\n",
      "522:\tlearn: 0.3545296\ttotal: 55.3s\tremaining: 50.4s\n",
      "523:\tlearn: 0.3543954\ttotal: 55.4s\tremaining: 50.4s\n",
      "524:\tlearn: 0.3540891\ttotal: 55.5s\tremaining: 50.2s\n",
      "525:\tlearn: 0.3539113\ttotal: 55.6s\tremaining: 50.1s\n",
      "526:\tlearn: 0.3538169\ttotal: 55.7s\tremaining: 50s\n",
      "527:\tlearn: 0.3536555\ttotal: 55.8s\tremaining: 49.9s\n",
      "528:\tlearn: 0.3534983\ttotal: 56s\tremaining: 49.8s\n",
      "529:\tlearn: 0.3533747\ttotal: 56.1s\tremaining: 49.7s\n",
      "530:\tlearn: 0.3532545\ttotal: 56.2s\tremaining: 49.7s\n",
      "531:\tlearn: 0.3528833\ttotal: 56.3s\tremaining: 49.6s\n",
      "532:\tlearn: 0.3527954\ttotal: 56.4s\tremaining: 49.4s\n",
      "533:\tlearn: 0.3526656\ttotal: 56.6s\tremaining: 49.4s\n",
      "534:\tlearn: 0.3522239\ttotal: 56.7s\tremaining: 49.3s\n",
      "535:\tlearn: 0.3521442\ttotal: 56.8s\tremaining: 49.2s\n",
      "536:\tlearn: 0.3520954\ttotal: 56.9s\tremaining: 49s\n",
      "537:\tlearn: 0.3520122\ttotal: 57s\tremaining: 48.9s\n",
      "538:\tlearn: 0.3519448\ttotal: 57.1s\tremaining: 48.8s\n",
      "539:\tlearn: 0.3517996\ttotal: 57.2s\tremaining: 48.7s\n",
      "540:\tlearn: 0.3515113\ttotal: 57.3s\tremaining: 48.6s\n",
      "541:\tlearn: 0.3514446\ttotal: 57.4s\tremaining: 48.5s\n",
      "542:\tlearn: 0.3512552\ttotal: 57.5s\tremaining: 48.4s\n",
      "543:\tlearn: 0.3511802\ttotal: 57.6s\tremaining: 48.3s\n",
      "544:\tlearn: 0.3511154\ttotal: 57.7s\tremaining: 48.1s\n",
      "545:\tlearn: 0.3510182\ttotal: 57.8s\tremaining: 48s\n",
      "546:\tlearn: 0.3508131\ttotal: 57.9s\tremaining: 47.9s\n",
      "547:\tlearn: 0.3507680\ttotal: 58s\tremaining: 47.8s\n",
      "548:\tlearn: 0.3506757\ttotal: 58.1s\tremaining: 47.7s\n",
      "549:\tlearn: 0.3506435\ttotal: 58.1s\tremaining: 47.6s\n",
      "550:\tlearn: 0.3506000\ttotal: 58.2s\tremaining: 47.5s\n",
      "551:\tlearn: 0.3505429\ttotal: 58.3s\tremaining: 47.3s\n",
      "552:\tlearn: 0.3503036\ttotal: 58.4s\tremaining: 47.2s\n",
      "553:\tlearn: 0.3502385\ttotal: 58.5s\tremaining: 47.1s\n",
      "554:\tlearn: 0.3501195\ttotal: 58.6s\tremaining: 46.9s\n",
      "555:\tlearn: 0.3499250\ttotal: 58.7s\tremaining: 46.9s\n",
      "556:\tlearn: 0.3497979\ttotal: 58.8s\tremaining: 46.8s\n",
      "557:\tlearn: 0.3497258\ttotal: 58.9s\tremaining: 46.6s\n",
      "558:\tlearn: 0.3496750\ttotal: 59s\tremaining: 46.5s\n",
      "559:\tlearn: 0.3496055\ttotal: 59.1s\tremaining: 46.4s\n",
      "560:\tlearn: 0.3492925\ttotal: 59.2s\tremaining: 46.3s\n",
      "561:\tlearn: 0.3492478\ttotal: 59.3s\tremaining: 46.2s\n",
      "562:\tlearn: 0.3491750\ttotal: 59.4s\tremaining: 46.1s\n",
      "563:\tlearn: 0.3490917\ttotal: 59.5s\tremaining: 46s\n",
      "564:\tlearn: 0.3488666\ttotal: 59.6s\tremaining: 45.9s\n",
      "565:\tlearn: 0.3488170\ttotal: 59.7s\tremaining: 45.8s\n",
      "566:\tlearn: 0.3487122\ttotal: 59.8s\tremaining: 45.7s\n",
      "567:\tlearn: 0.3485869\ttotal: 60s\tremaining: 45.6s\n",
      "568:\tlearn: 0.3485461\ttotal: 1m\tremaining: 45.5s\n",
      "569:\tlearn: 0.3485053\ttotal: 1m\tremaining: 45.3s\n",
      "570:\tlearn: 0.3484393\ttotal: 1m\tremaining: 45.2s\n",
      "571:\tlearn: 0.3483719\ttotal: 1m\tremaining: 45.1s\n",
      "572:\tlearn: 0.3483279\ttotal: 1m\tremaining: 45s\n",
      "573:\tlearn: 0.3482755\ttotal: 1m\tremaining: 44.9s\n",
      "574:\tlearn: 0.3482142\ttotal: 1m\tremaining: 44.7s\n",
      "575:\tlearn: 0.3481177\ttotal: 1m\tremaining: 44.7s\n",
      "576:\tlearn: 0.3480632\ttotal: 1m\tremaining: 44.5s\n",
      "577:\tlearn: 0.3480211\ttotal: 1m\tremaining: 44.4s\n",
      "578:\tlearn: 0.3479477\ttotal: 1m\tremaining: 44.3s\n",
      "579:\tlearn: 0.3478375\ttotal: 1m 1s\tremaining: 44.2s\n",
      "580:\tlearn: 0.3476822\ttotal: 1m 1s\tremaining: 44.1s\n",
      "581:\tlearn: 0.3475678\ttotal: 1m 1s\tremaining: 44s\n",
      "582:\tlearn: 0.3474340\ttotal: 1m 1s\tremaining: 43.9s\n",
      "583:\tlearn: 0.3473869\ttotal: 1m 1s\tremaining: 43.8s\n",
      "584:\tlearn: 0.3472926\ttotal: 1m 1s\tremaining: 43.7s\n",
      "585:\tlearn: 0.3472048\ttotal: 1m 1s\tremaining: 43.6s\n",
      "586:\tlearn: 0.3469658\ttotal: 1m 1s\tremaining: 43.5s\n",
      "587:\tlearn: 0.3468773\ttotal: 1m 1s\tremaining: 43.4s\n",
      "588:\tlearn: 0.3467798\ttotal: 1m 2s\tremaining: 43.3s\n",
      "589:\tlearn: 0.3466737\ttotal: 1m 2s\tremaining: 43.2s\n",
      "590:\tlearn: 0.3465440\ttotal: 1m 2s\tremaining: 43.1s\n",
      "591:\tlearn: 0.3464858\ttotal: 1m 2s\tremaining: 43s\n",
      "592:\tlearn: 0.3464172\ttotal: 1m 2s\tremaining: 42.8s\n",
      "593:\tlearn: 0.3462114\ttotal: 1m 2s\tremaining: 42.7s\n",
      "594:\tlearn: 0.3460869\ttotal: 1m 2s\tremaining: 42.7s\n",
      "595:\tlearn: 0.3459126\ttotal: 1m 2s\tremaining: 42.6s\n",
      "596:\tlearn: 0.3457890\ttotal: 1m 2s\tremaining: 42.5s\n",
      "597:\tlearn: 0.3456495\ttotal: 1m 3s\tremaining: 42.4s\n",
      "598:\tlearn: 0.3455233\ttotal: 1m 3s\tremaining: 42.3s\n",
      "599:\tlearn: 0.3454747\ttotal: 1m 3s\tremaining: 42.2s\n",
      "600:\tlearn: 0.3454283\ttotal: 1m 3s\tremaining: 42s\n",
      "601:\tlearn: 0.3453477\ttotal: 1m 3s\tremaining: 41.9s\n",
      "602:\tlearn: 0.3452223\ttotal: 1m 3s\tremaining: 41.8s\n",
      "603:\tlearn: 0.3450592\ttotal: 1m 3s\tremaining: 41.7s\n",
      "604:\tlearn: 0.3449645\ttotal: 1m 3s\tremaining: 41.6s\n",
      "605:\tlearn: 0.3448950\ttotal: 1m 3s\tremaining: 41.5s\n",
      "606:\tlearn: 0.3448045\ttotal: 1m 3s\tremaining: 41.4s\n",
      "607:\tlearn: 0.3447336\ttotal: 1m 4s\tremaining: 41.3s\n",
      "608:\tlearn: 0.3446591\ttotal: 1m 4s\tremaining: 41.2s\n",
      "609:\tlearn: 0.3445171\ttotal: 1m 4s\tremaining: 41.1s\n",
      "610:\tlearn: 0.3444506\ttotal: 1m 4s\tremaining: 41s\n",
      "611:\tlearn: 0.3443609\ttotal: 1m 4s\tremaining: 40.9s\n",
      "612:\tlearn: 0.3441999\ttotal: 1m 4s\tremaining: 40.8s\n",
      "613:\tlearn: 0.3440946\ttotal: 1m 4s\tremaining: 40.7s\n",
      "614:\tlearn: 0.3440098\ttotal: 1m 4s\tremaining: 40.6s\n",
      "615:\tlearn: 0.3437634\ttotal: 1m 4s\tremaining: 40.5s\n",
      "616:\tlearn: 0.3436843\ttotal: 1m 5s\tremaining: 40.4s\n",
      "617:\tlearn: 0.3434618\ttotal: 1m 5s\tremaining: 40.2s\n",
      "618:\tlearn: 0.3433796\ttotal: 1m 5s\tremaining: 40.1s\n",
      "619:\tlearn: 0.3432427\ttotal: 1m 5s\tremaining: 40s\n",
      "620:\tlearn: 0.3431334\ttotal: 1m 5s\tremaining: 39.9s\n",
      "621:\tlearn: 0.3430757\ttotal: 1m 5s\tremaining: 39.8s\n",
      "622:\tlearn: 0.3428754\ttotal: 1m 5s\tremaining: 39.7s\n",
      "623:\tlearn: 0.3428027\ttotal: 1m 5s\tremaining: 39.6s\n",
      "624:\tlearn: 0.3427213\ttotal: 1m 5s\tremaining: 39.5s\n",
      "625:\tlearn: 0.3426427\ttotal: 1m 5s\tremaining: 39.4s\n",
      "626:\tlearn: 0.3425832\ttotal: 1m 6s\tremaining: 39.3s\n",
      "627:\tlearn: 0.3424167\ttotal: 1m 6s\tremaining: 39.2s\n",
      "628:\tlearn: 0.3423063\ttotal: 1m 6s\tremaining: 39.1s\n",
      "629:\tlearn: 0.3422565\ttotal: 1m 6s\tremaining: 39s\n",
      "630:\tlearn: 0.3420391\ttotal: 1m 6s\tremaining: 38.9s\n",
      "631:\tlearn: 0.3419552\ttotal: 1m 6s\tremaining: 38.8s\n",
      "632:\tlearn: 0.3418734\ttotal: 1m 6s\tremaining: 38.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633:\tlearn: 0.3418191\ttotal: 1m 6s\tremaining: 38.6s\n",
      "634:\tlearn: 0.3417493\ttotal: 1m 6s\tremaining: 38.5s\n",
      "635:\tlearn: 0.3416673\ttotal: 1m 7s\tremaining: 38.4s\n",
      "636:\tlearn: 0.3415754\ttotal: 1m 7s\tremaining: 38.3s\n",
      "637:\tlearn: 0.3414057\ttotal: 1m 7s\tremaining: 38.2s\n",
      "638:\tlearn: 0.3412816\ttotal: 1m 7s\tremaining: 38s\n",
      "639:\tlearn: 0.3411607\ttotal: 1m 7s\tremaining: 37.9s\n",
      "640:\tlearn: 0.3410383\ttotal: 1m 7s\tremaining: 37.8s\n",
      "641:\tlearn: 0.3409430\ttotal: 1m 7s\tremaining: 37.7s\n",
      "642:\tlearn: 0.3408939\ttotal: 1m 7s\tremaining: 37.6s\n",
      "643:\tlearn: 0.3408406\ttotal: 1m 7s\tremaining: 37.5s\n",
      "644:\tlearn: 0.3407563\ttotal: 1m 7s\tremaining: 37.4s\n",
      "645:\tlearn: 0.3406704\ttotal: 1m 8s\tremaining: 37.3s\n",
      "646:\tlearn: 0.3405993\ttotal: 1m 8s\tremaining: 37.2s\n",
      "647:\tlearn: 0.3403150\ttotal: 1m 8s\tremaining: 37.1s\n",
      "648:\tlearn: 0.3402657\ttotal: 1m 8s\tremaining: 37s\n",
      "649:\tlearn: 0.3401971\ttotal: 1m 8s\tremaining: 36.8s\n",
      "650:\tlearn: 0.3401349\ttotal: 1m 8s\tremaining: 36.7s\n",
      "651:\tlearn: 0.3400598\ttotal: 1m 8s\tremaining: 36.6s\n",
      "652:\tlearn: 0.3399734\ttotal: 1m 8s\tremaining: 36.5s\n",
      "653:\tlearn: 0.3399043\ttotal: 1m 8s\tremaining: 36.4s\n",
      "654:\tlearn: 0.3398636\ttotal: 1m 8s\tremaining: 36.3s\n",
      "655:\tlearn: 0.3398151\ttotal: 1m 8s\tremaining: 36.1s\n",
      "656:\tlearn: 0.3397489\ttotal: 1m 9s\tremaining: 36s\n",
      "657:\tlearn: 0.3396657\ttotal: 1m 9s\tremaining: 35.9s\n",
      "658:\tlearn: 0.3394905\ttotal: 1m 9s\tremaining: 35.8s\n",
      "659:\tlearn: 0.3393845\ttotal: 1m 9s\tremaining: 35.7s\n",
      "660:\tlearn: 0.3393207\ttotal: 1m 9s\tremaining: 35.6s\n",
      "661:\tlearn: 0.3392064\ttotal: 1m 9s\tremaining: 35.5s\n",
      "662:\tlearn: 0.3391521\ttotal: 1m 9s\tremaining: 35.4s\n",
      "663:\tlearn: 0.3390456\ttotal: 1m 9s\tremaining: 35.3s\n",
      "664:\tlearn: 0.3389870\ttotal: 1m 9s\tremaining: 35.2s\n",
      "665:\tlearn: 0.3386927\ttotal: 1m 9s\tremaining: 35.1s\n",
      "666:\tlearn: 0.3386238\ttotal: 1m 10s\tremaining: 35s\n",
      "667:\tlearn: 0.3385671\ttotal: 1m 10s\tremaining: 34.9s\n",
      "668:\tlearn: 0.3384853\ttotal: 1m 10s\tremaining: 34.8s\n",
      "669:\tlearn: 0.3382442\ttotal: 1m 10s\tremaining: 34.7s\n",
      "670:\tlearn: 0.3381510\ttotal: 1m 10s\tremaining: 34.5s\n",
      "671:\tlearn: 0.3380244\ttotal: 1m 10s\tremaining: 34.4s\n",
      "672:\tlearn: 0.3378684\ttotal: 1m 10s\tremaining: 34.3s\n",
      "673:\tlearn: 0.3377989\ttotal: 1m 10s\tremaining: 34.2s\n",
      "674:\tlearn: 0.3377397\ttotal: 1m 10s\tremaining: 34.1s\n",
      "675:\tlearn: 0.3375062\ttotal: 1m 10s\tremaining: 34s\n",
      "676:\tlearn: 0.3374447\ttotal: 1m 11s\tremaining: 33.9s\n",
      "677:\tlearn: 0.3373219\ttotal: 1m 11s\tremaining: 33.8s\n",
      "678:\tlearn: 0.3371988\ttotal: 1m 11s\tremaining: 33.7s\n",
      "679:\tlearn: 0.3370262\ttotal: 1m 11s\tremaining: 33.6s\n",
      "680:\tlearn: 0.3369648\ttotal: 1m 11s\tremaining: 33.5s\n",
      "681:\tlearn: 0.3369112\ttotal: 1m 11s\tremaining: 33.4s\n",
      "682:\tlearn: 0.3368383\ttotal: 1m 11s\tremaining: 33.3s\n",
      "683:\tlearn: 0.3367386\ttotal: 1m 11s\tremaining: 33.2s\n",
      "684:\tlearn: 0.3366282\ttotal: 1m 11s\tremaining: 33s\n",
      "685:\tlearn: 0.3365614\ttotal: 1m 11s\tremaining: 32.9s\n",
      "686:\tlearn: 0.3364903\ttotal: 1m 12s\tremaining: 32.8s\n",
      "687:\tlearn: 0.3364585\ttotal: 1m 12s\tremaining: 32.7s\n",
      "688:\tlearn: 0.3364355\ttotal: 1m 12s\tremaining: 32.6s\n",
      "689:\tlearn: 0.3363840\ttotal: 1m 12s\tremaining: 32.5s\n",
      "690:\tlearn: 0.3362477\ttotal: 1m 12s\tremaining: 32.4s\n",
      "691:\tlearn: 0.3361741\ttotal: 1m 12s\tremaining: 32.2s\n",
      "692:\tlearn: 0.3361179\ttotal: 1m 12s\tremaining: 32.1s\n",
      "693:\tlearn: 0.3360133\ttotal: 1m 12s\tremaining: 32s\n",
      "694:\tlearn: 0.3359353\ttotal: 1m 12s\tremaining: 31.9s\n",
      "695:\tlearn: 0.3357957\ttotal: 1m 12s\tremaining: 31.8s\n",
      "696:\tlearn: 0.3355755\ttotal: 1m 13s\tremaining: 31.7s\n",
      "697:\tlearn: 0.3354927\ttotal: 1m 13s\tremaining: 31.6s\n",
      "698:\tlearn: 0.3354235\ttotal: 1m 13s\tremaining: 31.5s\n",
      "699:\tlearn: 0.3353632\ttotal: 1m 13s\tremaining: 31.4s\n",
      "700:\tlearn: 0.3350786\ttotal: 1m 13s\tremaining: 31.3s\n",
      "701:\tlearn: 0.3350122\ttotal: 1m 13s\tremaining: 31.2s\n",
      "702:\tlearn: 0.3349125\ttotal: 1m 13s\tremaining: 31.1s\n",
      "703:\tlearn: 0.3347430\ttotal: 1m 13s\tremaining: 31s\n",
      "704:\tlearn: 0.3346465\ttotal: 1m 13s\tremaining: 30.9s\n",
      "705:\tlearn: 0.3345340\ttotal: 1m 13s\tremaining: 30.8s\n",
      "706:\tlearn: 0.3344330\ttotal: 1m 14s\tremaining: 30.7s\n",
      "707:\tlearn: 0.3343594\ttotal: 1m 14s\tremaining: 30.6s\n",
      "708:\tlearn: 0.3342968\ttotal: 1m 14s\tremaining: 30.5s\n",
      "709:\tlearn: 0.3342600\ttotal: 1m 14s\tremaining: 30.4s\n",
      "710:\tlearn: 0.3341903\ttotal: 1m 14s\tremaining: 30.3s\n",
      "711:\tlearn: 0.3341279\ttotal: 1m 14s\tremaining: 30.2s\n",
      "712:\tlearn: 0.3340356\ttotal: 1m 14s\tremaining: 30.1s\n",
      "713:\tlearn: 0.3339821\ttotal: 1m 14s\tremaining: 29.9s\n",
      "714:\tlearn: 0.3338373\ttotal: 1m 14s\tremaining: 29.8s\n",
      "715:\tlearn: 0.3337948\ttotal: 1m 14s\tremaining: 29.7s\n",
      "716:\tlearn: 0.3337294\ttotal: 1m 15s\tremaining: 29.6s\n",
      "717:\tlearn: 0.3337058\ttotal: 1m 15s\tremaining: 29.5s\n",
      "718:\tlearn: 0.3335936\ttotal: 1m 15s\tremaining: 29.4s\n",
      "719:\tlearn: 0.3334637\ttotal: 1m 15s\tremaining: 29.3s\n",
      "720:\tlearn: 0.3334028\ttotal: 1m 15s\tremaining: 29.2s\n",
      "721:\tlearn: 0.3333753\ttotal: 1m 15s\tremaining: 29.1s\n",
      "722:\tlearn: 0.3332830\ttotal: 1m 15s\tremaining: 29s\n",
      "723:\tlearn: 0.3332351\ttotal: 1m 15s\tremaining: 28.9s\n",
      "724:\tlearn: 0.3331211\ttotal: 1m 15s\tremaining: 28.8s\n",
      "725:\tlearn: 0.3330506\ttotal: 1m 16s\tremaining: 28.7s\n",
      "726:\tlearn: 0.3328755\ttotal: 1m 16s\tremaining: 28.6s\n",
      "727:\tlearn: 0.3327784\ttotal: 1m 16s\tremaining: 28.5s\n",
      "728:\tlearn: 0.3324873\ttotal: 1m 16s\tremaining: 28.4s\n",
      "729:\tlearn: 0.3324142\ttotal: 1m 16s\tremaining: 28.3s\n",
      "730:\tlearn: 0.3323510\ttotal: 1m 16s\tremaining: 28.2s\n",
      "731:\tlearn: 0.3322479\ttotal: 1m 16s\tremaining: 28.1s\n",
      "732:\tlearn: 0.3320843\ttotal: 1m 16s\tremaining: 28s\n",
      "733:\tlearn: 0.3319446\ttotal: 1m 16s\tremaining: 27.9s\n",
      "734:\tlearn: 0.3317930\ttotal: 1m 16s\tremaining: 27.8s\n",
      "735:\tlearn: 0.3315810\ttotal: 1m 17s\tremaining: 27.7s\n",
      "736:\tlearn: 0.3315337\ttotal: 1m 17s\tremaining: 27.6s\n",
      "737:\tlearn: 0.3313758\ttotal: 1m 17s\tremaining: 27.5s\n",
      "738:\tlearn: 0.3312421\ttotal: 1m 17s\tremaining: 27.4s\n",
      "739:\tlearn: 0.3311863\ttotal: 1m 17s\tremaining: 27.2s\n",
      "740:\tlearn: 0.3311163\ttotal: 1m 17s\tremaining: 27.1s\n",
      "741:\tlearn: 0.3310002\ttotal: 1m 17s\tremaining: 27s\n",
      "742:\tlearn: 0.3309149\ttotal: 1m 17s\tremaining: 26.9s\n",
      "743:\tlearn: 0.3308466\ttotal: 1m 17s\tremaining: 26.8s\n",
      "744:\tlearn: 0.3308094\ttotal: 1m 18s\tremaining: 26.7s\n",
      "745:\tlearn: 0.3307263\ttotal: 1m 18s\tremaining: 26.6s\n",
      "746:\tlearn: 0.3306350\ttotal: 1m 18s\tremaining: 26.5s\n",
      "747:\tlearn: 0.3305486\ttotal: 1m 18s\tremaining: 26.4s\n",
      "748:\tlearn: 0.3304862\ttotal: 1m 18s\tremaining: 26.3s\n",
      "749:\tlearn: 0.3304327\ttotal: 1m 18s\tremaining: 26.2s\n",
      "750:\tlearn: 0.3303764\ttotal: 1m 18s\tremaining: 26.1s\n",
      "751:\tlearn: 0.3303010\ttotal: 1m 18s\tremaining: 26s\n",
      "752:\tlearn: 0.3302368\ttotal: 1m 18s\tremaining: 25.9s\n",
      "753:\tlearn: 0.3301209\ttotal: 1m 18s\tremaining: 25.8s\n",
      "754:\tlearn: 0.3300591\ttotal: 1m 19s\tremaining: 25.7s\n",
      "755:\tlearn: 0.3300322\ttotal: 1m 19s\tremaining: 25.5s\n",
      "756:\tlearn: 0.3299320\ttotal: 1m 19s\tremaining: 25.4s\n",
      "757:\tlearn: 0.3296118\ttotal: 1m 19s\tremaining: 25.3s\n",
      "758:\tlearn: 0.3295348\ttotal: 1m 19s\tremaining: 25.2s\n",
      "759:\tlearn: 0.3294777\ttotal: 1m 19s\tremaining: 25.1s\n",
      "760:\tlearn: 0.3293545\ttotal: 1m 19s\tremaining: 25s\n",
      "761:\tlearn: 0.3293074\ttotal: 1m 19s\tremaining: 24.9s\n",
      "762:\tlearn: 0.3292501\ttotal: 1m 19s\tremaining: 24.8s\n",
      "763:\tlearn: 0.3291680\ttotal: 1m 19s\tremaining: 24.7s\n",
      "764:\tlearn: 0.3289920\ttotal: 1m 20s\tremaining: 24.6s\n",
      "765:\tlearn: 0.3289609\ttotal: 1m 20s\tremaining: 24.5s\n",
      "766:\tlearn: 0.3289432\ttotal: 1m 20s\tremaining: 24.4s\n",
      "767:\tlearn: 0.3288147\ttotal: 1m 20s\tremaining: 24.3s\n",
      "768:\tlearn: 0.3287509\ttotal: 1m 20s\tremaining: 24.2s\n",
      "769:\tlearn: 0.3286726\ttotal: 1m 20s\tremaining: 24.1s\n",
      "770:\tlearn: 0.3286266\ttotal: 1m 20s\tremaining: 24s\n",
      "771:\tlearn: 0.3285427\ttotal: 1m 20s\tremaining: 23.9s\n",
      "772:\tlearn: 0.3284631\ttotal: 1m 20s\tremaining: 23.8s\n",
      "773:\tlearn: 0.3283892\ttotal: 1m 21s\tremaining: 23.7s\n",
      "774:\tlearn: 0.3282709\ttotal: 1m 21s\tremaining: 23.6s\n",
      "775:\tlearn: 0.3281997\ttotal: 1m 21s\tremaining: 23.4s\n",
      "776:\tlearn: 0.3281153\ttotal: 1m 21s\tremaining: 23.3s\n",
      "777:\tlearn: 0.3280586\ttotal: 1m 21s\tremaining: 23.2s\n",
      "778:\tlearn: 0.3279591\ttotal: 1m 21s\tremaining: 23.1s\n",
      "779:\tlearn: 0.3277767\ttotal: 1m 21s\tremaining: 23s\n",
      "780:\tlearn: 0.3277221\ttotal: 1m 21s\tremaining: 22.9s\n",
      "781:\tlearn: 0.3276264\ttotal: 1m 21s\tremaining: 22.8s\n",
      "782:\tlearn: 0.3274600\ttotal: 1m 22s\tremaining: 22.7s\n",
      "783:\tlearn: 0.3273723\ttotal: 1m 22s\tremaining: 22.6s\n",
      "784:\tlearn: 0.3273262\ttotal: 1m 22s\tremaining: 22.5s\n",
      "785:\tlearn: 0.3272627\ttotal: 1m 22s\tremaining: 22.4s\n",
      "786:\tlearn: 0.3271996\ttotal: 1m 22s\tremaining: 22.3s\n",
      "787:\tlearn: 0.3271462\ttotal: 1m 22s\tremaining: 22.2s\n",
      "788:\tlearn: 0.3269571\ttotal: 1m 22s\tremaining: 22.1s\n",
      "789:\tlearn: 0.3268918\ttotal: 1m 22s\tremaining: 22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790:\tlearn: 0.3268288\ttotal: 1m 22s\tremaining: 21.9s\n",
      "791:\tlearn: 0.3266282\ttotal: 1m 22s\tremaining: 21.8s\n",
      "792:\tlearn: 0.3265380\ttotal: 1m 23s\tremaining: 21.7s\n",
      "793:\tlearn: 0.3264482\ttotal: 1m 23s\tremaining: 21.6s\n",
      "794:\tlearn: 0.3264441\ttotal: 1m 23s\tremaining: 21.5s\n",
      "795:\tlearn: 0.3263487\ttotal: 1m 23s\tremaining: 21.4s\n",
      "796:\tlearn: 0.3263066\ttotal: 1m 23s\tremaining: 21.3s\n",
      "797:\tlearn: 0.3262621\ttotal: 1m 23s\tremaining: 21.1s\n",
      "798:\tlearn: 0.3261644\ttotal: 1m 23s\tremaining: 21s\n",
      "799:\tlearn: 0.3261113\ttotal: 1m 23s\tremaining: 20.9s\n",
      "800:\tlearn: 0.3259943\ttotal: 1m 23s\tremaining: 20.8s\n",
      "801:\tlearn: 0.3258240\ttotal: 1m 23s\tremaining: 20.7s\n",
      "802:\tlearn: 0.3256917\ttotal: 1m 24s\tremaining: 20.6s\n",
      "803:\tlearn: 0.3255815\ttotal: 1m 24s\tremaining: 20.5s\n",
      "804:\tlearn: 0.3255212\ttotal: 1m 24s\tremaining: 20.4s\n",
      "805:\tlearn: 0.3254663\ttotal: 1m 24s\tremaining: 20.3s\n",
      "806:\tlearn: 0.3254165\ttotal: 1m 24s\tremaining: 20.2s\n",
      "807:\tlearn: 0.3252063\ttotal: 1m 24s\tremaining: 20.1s\n",
      "808:\tlearn: 0.3251159\ttotal: 1m 24s\tremaining: 20s\n",
      "809:\tlearn: 0.3250794\ttotal: 1m 24s\tremaining: 19.9s\n",
      "810:\tlearn: 0.3249975\ttotal: 1m 24s\tremaining: 19.8s\n",
      "811:\tlearn: 0.3249481\ttotal: 1m 24s\tremaining: 19.7s\n",
      "812:\tlearn: 0.3248720\ttotal: 1m 25s\tremaining: 19.6s\n",
      "813:\tlearn: 0.3248159\ttotal: 1m 25s\tremaining: 19.5s\n",
      "814:\tlearn: 0.3247078\ttotal: 1m 25s\tremaining: 19.4s\n",
      "815:\tlearn: 0.3246347\ttotal: 1m 25s\tremaining: 19.3s\n",
      "816:\tlearn: 0.3245973\ttotal: 1m 25s\tremaining: 19.1s\n",
      "817:\tlearn: 0.3245235\ttotal: 1m 25s\tremaining: 19s\n",
      "818:\tlearn: 0.3244730\ttotal: 1m 25s\tremaining: 18.9s\n",
      "819:\tlearn: 0.3244018\ttotal: 1m 25s\tremaining: 18.8s\n",
      "820:\tlearn: 0.3242826\ttotal: 1m 25s\tremaining: 18.7s\n",
      "821:\tlearn: 0.3242258\ttotal: 1m 25s\tremaining: 18.6s\n",
      "822:\tlearn: 0.3241666\ttotal: 1m 26s\tremaining: 18.5s\n",
      "823:\tlearn: 0.3240724\ttotal: 1m 26s\tremaining: 18.4s\n",
      "824:\tlearn: 0.3240065\ttotal: 1m 26s\tremaining: 18.3s\n",
      "825:\tlearn: 0.3238904\ttotal: 1m 26s\tremaining: 18.2s\n",
      "826:\tlearn: 0.3238690\ttotal: 1m 26s\tremaining: 18.1s\n",
      "827:\tlearn: 0.3238055\ttotal: 1m 26s\tremaining: 18s\n",
      "828:\tlearn: 0.3237750\ttotal: 1m 26s\tremaining: 17.9s\n",
      "829:\tlearn: 0.3236637\ttotal: 1m 26s\tremaining: 17.8s\n",
      "830:\tlearn: 0.3236228\ttotal: 1m 26s\tremaining: 17.6s\n",
      "831:\tlearn: 0.3235035\ttotal: 1m 26s\tremaining: 17.5s\n",
      "832:\tlearn: 0.3234540\ttotal: 1m 26s\tremaining: 17.4s\n",
      "833:\tlearn: 0.3233692\ttotal: 1m 27s\tremaining: 17.3s\n",
      "834:\tlearn: 0.3232932\ttotal: 1m 27s\tremaining: 17.2s\n",
      "835:\tlearn: 0.3232321\ttotal: 1m 27s\tremaining: 17.1s\n",
      "836:\tlearn: 0.3231781\ttotal: 1m 27s\tremaining: 17s\n",
      "837:\tlearn: 0.3231203\ttotal: 1m 27s\tremaining: 16.9s\n",
      "838:\tlearn: 0.3230574\ttotal: 1m 27s\tremaining: 16.8s\n",
      "839:\tlearn: 0.3229721\ttotal: 1m 27s\tremaining: 16.7s\n",
      "840:\tlearn: 0.3229408\ttotal: 1m 27s\tremaining: 16.6s\n",
      "841:\tlearn: 0.3228770\ttotal: 1m 27s\tremaining: 16.5s\n",
      "842:\tlearn: 0.3228051\ttotal: 1m 27s\tremaining: 16.4s\n",
      "843:\tlearn: 0.3227134\ttotal: 1m 28s\tremaining: 16.3s\n",
      "844:\tlearn: 0.3226697\ttotal: 1m 28s\tremaining: 16.2s\n",
      "845:\tlearn: 0.3226264\ttotal: 1m 28s\tremaining: 16.1s\n",
      "846:\tlearn: 0.3225603\ttotal: 1m 28s\tremaining: 16s\n",
      "847:\tlearn: 0.3224717\ttotal: 1m 28s\tremaining: 15.9s\n",
      "848:\tlearn: 0.3224139\ttotal: 1m 28s\tremaining: 15.8s\n",
      "849:\tlearn: 0.3223459\ttotal: 1m 28s\tremaining: 15.7s\n",
      "850:\tlearn: 0.3222757\ttotal: 1m 28s\tremaining: 15.6s\n",
      "851:\tlearn: 0.3222225\ttotal: 1m 28s\tremaining: 15.5s\n",
      "852:\tlearn: 0.3221323\ttotal: 1m 29s\tremaining: 15.4s\n",
      "853:\tlearn: 0.3220931\ttotal: 1m 29s\tremaining: 15.2s\n",
      "854:\tlearn: 0.3219751\ttotal: 1m 29s\tremaining: 15.1s\n",
      "855:\tlearn: 0.3219279\ttotal: 1m 29s\tremaining: 15s\n",
      "856:\tlearn: 0.3218213\ttotal: 1m 29s\tremaining: 14.9s\n",
      "857:\tlearn: 0.3218095\ttotal: 1m 29s\tremaining: 14.8s\n",
      "858:\tlearn: 0.3217769\ttotal: 1m 29s\tremaining: 14.7s\n",
      "859:\tlearn: 0.3217132\ttotal: 1m 29s\tremaining: 14.6s\n",
      "860:\tlearn: 0.3216418\ttotal: 1m 29s\tremaining: 14.5s\n",
      "861:\tlearn: 0.3216014\ttotal: 1m 29s\tremaining: 14.4s\n",
      "862:\tlearn: 0.3215632\ttotal: 1m 30s\tremaining: 14.3s\n",
      "863:\tlearn: 0.3215190\ttotal: 1m 30s\tremaining: 14.2s\n",
      "864:\tlearn: 0.3214460\ttotal: 1m 30s\tremaining: 14.1s\n",
      "865:\tlearn: 0.3213817\ttotal: 1m 30s\tremaining: 14s\n",
      "866:\tlearn: 0.3213103\ttotal: 1m 30s\tremaining: 13.9s\n",
      "867:\tlearn: 0.3212054\ttotal: 1m 30s\tremaining: 13.8s\n",
      "868:\tlearn: 0.3211163\ttotal: 1m 30s\tremaining: 13.7s\n",
      "869:\tlearn: 0.3210920\ttotal: 1m 30s\tremaining: 13.6s\n",
      "870:\tlearn: 0.3210659\ttotal: 1m 30s\tremaining: 13.5s\n",
      "871:\tlearn: 0.3209497\ttotal: 1m 30s\tremaining: 13.3s\n",
      "872:\tlearn: 0.3208534\ttotal: 1m 31s\tremaining: 13.2s\n",
      "873:\tlearn: 0.3208201\ttotal: 1m 31s\tremaining: 13.1s\n",
      "874:\tlearn: 0.3207702\ttotal: 1m 31s\tremaining: 13s\n",
      "875:\tlearn: 0.3206458\ttotal: 1m 31s\tremaining: 12.9s\n",
      "876:\tlearn: 0.3205894\ttotal: 1m 31s\tremaining: 12.8s\n",
      "877:\tlearn: 0.3205534\ttotal: 1m 31s\tremaining: 12.7s\n",
      "878:\tlearn: 0.3205033\ttotal: 1m 31s\tremaining: 12.6s\n",
      "879:\tlearn: 0.3204242\ttotal: 1m 31s\tremaining: 12.5s\n",
      "880:\tlearn: 0.3203432\ttotal: 1m 31s\tremaining: 12.4s\n",
      "881:\tlearn: 0.3202847\ttotal: 1m 31s\tremaining: 12.3s\n",
      "882:\tlearn: 0.3202246\ttotal: 1m 31s\tremaining: 12.2s\n",
      "883:\tlearn: 0.3201839\ttotal: 1m 32s\tremaining: 12.1s\n",
      "884:\tlearn: 0.3200448\ttotal: 1m 32s\tremaining: 12s\n",
      "885:\tlearn: 0.3199945\ttotal: 1m 32s\tremaining: 11.9s\n",
      "886:\tlearn: 0.3199334\ttotal: 1m 32s\tremaining: 11.8s\n",
      "887:\tlearn: 0.3199021\ttotal: 1m 32s\tremaining: 11.7s\n",
      "888:\tlearn: 0.3198052\ttotal: 1m 32s\tremaining: 11.6s\n",
      "889:\tlearn: 0.3197494\ttotal: 1m 32s\tremaining: 11.5s\n",
      "890:\tlearn: 0.3196731\ttotal: 1m 32s\tremaining: 11.3s\n",
      "891:\tlearn: 0.3196443\ttotal: 1m 32s\tremaining: 11.2s\n",
      "892:\tlearn: 0.3195810\ttotal: 1m 32s\tremaining: 11.1s\n",
      "893:\tlearn: 0.3193762\ttotal: 1m 33s\tremaining: 11s\n",
      "894:\tlearn: 0.3193129\ttotal: 1m 33s\tremaining: 10.9s\n",
      "895:\tlearn: 0.3192584\ttotal: 1m 33s\tremaining: 10.8s\n",
      "896:\tlearn: 0.3192202\ttotal: 1m 33s\tremaining: 10.7s\n",
      "897:\tlearn: 0.3191104\ttotal: 1m 33s\tremaining: 10.6s\n",
      "898:\tlearn: 0.3190532\ttotal: 1m 33s\tremaining: 10.5s\n",
      "899:\tlearn: 0.3190027\ttotal: 1m 33s\tremaining: 10.4s\n",
      "900:\tlearn: 0.3189308\ttotal: 1m 33s\tremaining: 10.3s\n",
      "901:\tlearn: 0.3188718\ttotal: 1m 33s\tremaining: 10.2s\n",
      "902:\tlearn: 0.3187917\ttotal: 1m 34s\tremaining: 10.1s\n",
      "903:\tlearn: 0.3186854\ttotal: 1m 34s\tremaining: 9.99s\n",
      "904:\tlearn: 0.3186511\ttotal: 1m 34s\tremaining: 9.89s\n",
      "905:\tlearn: 0.3185878\ttotal: 1m 34s\tremaining: 9.79s\n",
      "906:\tlearn: 0.3185215\ttotal: 1m 34s\tremaining: 9.69s\n",
      "907:\tlearn: 0.3184133\ttotal: 1m 34s\tremaining: 9.59s\n",
      "908:\tlearn: 0.3183499\ttotal: 1m 34s\tremaining: 9.48s\n",
      "909:\tlearn: 0.3182600\ttotal: 1m 34s\tremaining: 9.38s\n",
      "910:\tlearn: 0.3181490\ttotal: 1m 34s\tremaining: 9.27s\n",
      "911:\tlearn: 0.3180693\ttotal: 1m 35s\tremaining: 9.17s\n",
      "912:\tlearn: 0.3180038\ttotal: 1m 35s\tremaining: 9.06s\n",
      "913:\tlearn: 0.3179629\ttotal: 1m 35s\tremaining: 8.96s\n",
      "914:\tlearn: 0.3179105\ttotal: 1m 35s\tremaining: 8.86s\n",
      "915:\tlearn: 0.3178314\ttotal: 1m 35s\tremaining: 8.75s\n",
      "916:\tlearn: 0.3177461\ttotal: 1m 35s\tremaining: 8.65s\n",
      "917:\tlearn: 0.3176395\ttotal: 1m 35s\tremaining: 8.55s\n",
      "918:\tlearn: 0.3175642\ttotal: 1m 35s\tremaining: 8.45s\n",
      "919:\tlearn: 0.3174899\ttotal: 1m 35s\tremaining: 8.34s\n",
      "920:\tlearn: 0.3174153\ttotal: 1m 36s\tremaining: 8.24s\n",
      "921:\tlearn: 0.3173754\ttotal: 1m 36s\tremaining: 8.13s\n",
      "922:\tlearn: 0.3172912\ttotal: 1m 36s\tremaining: 8.02s\n",
      "923:\tlearn: 0.3172221\ttotal: 1m 36s\tremaining: 7.92s\n",
      "924:\tlearn: 0.3171209\ttotal: 1m 36s\tremaining: 7.82s\n",
      "925:\tlearn: 0.3170380\ttotal: 1m 36s\tremaining: 7.71s\n",
      "926:\tlearn: 0.3169623\ttotal: 1m 36s\tremaining: 7.61s\n",
      "927:\tlearn: 0.3169161\ttotal: 1m 36s\tremaining: 7.5s\n",
      "928:\tlearn: 0.3168210\ttotal: 1m 36s\tremaining: 7.4s\n",
      "929:\tlearn: 0.3167871\ttotal: 1m 36s\tremaining: 7.3s\n",
      "930:\tlearn: 0.3166987\ttotal: 1m 37s\tremaining: 7.2s\n",
      "931:\tlearn: 0.3166394\ttotal: 1m 37s\tremaining: 7.09s\n",
      "932:\tlearn: 0.3165862\ttotal: 1m 37s\tremaining: 6.99s\n",
      "933:\tlearn: 0.3164939\ttotal: 1m 37s\tremaining: 6.88s\n",
      "934:\tlearn: 0.3164562\ttotal: 1m 37s\tremaining: 6.78s\n",
      "935:\tlearn: 0.3163574\ttotal: 1m 37s\tremaining: 6.67s\n",
      "936:\tlearn: 0.3163044\ttotal: 1m 37s\tremaining: 6.57s\n",
      "937:\tlearn: 0.3162498\ttotal: 1m 37s\tremaining: 6.47s\n",
      "938:\tlearn: 0.3161712\ttotal: 1m 37s\tremaining: 6.36s\n",
      "939:\tlearn: 0.3161137\ttotal: 1m 38s\tremaining: 6.26s\n",
      "940:\tlearn: 0.3160571\ttotal: 1m 38s\tremaining: 6.16s\n",
      "941:\tlearn: 0.3160027\ttotal: 1m 38s\tremaining: 6.05s\n",
      "942:\tlearn: 0.3159418\ttotal: 1m 38s\tremaining: 5.95s\n",
      "943:\tlearn: 0.3158878\ttotal: 1m 38s\tremaining: 5.84s\n",
      "944:\tlearn: 0.3158350\ttotal: 1m 38s\tremaining: 5.74s\n",
      "945:\tlearn: 0.3157792\ttotal: 1m 38s\tremaining: 5.63s\n",
      "946:\tlearn: 0.3157253\ttotal: 1m 38s\tremaining: 5.53s\n",
      "947:\tlearn: 0.3156763\ttotal: 1m 38s\tremaining: 5.42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948:\tlearn: 0.3155834\ttotal: 1m 38s\tremaining: 5.32s\n",
      "949:\tlearn: 0.3155316\ttotal: 1m 39s\tremaining: 5.21s\n",
      "950:\tlearn: 0.3154673\ttotal: 1m 39s\tremaining: 5.11s\n",
      "951:\tlearn: 0.3153995\ttotal: 1m 39s\tremaining: 5s\n",
      "952:\tlearn: 0.3153477\ttotal: 1m 39s\tremaining: 4.9s\n",
      "953:\tlearn: 0.3152786\ttotal: 1m 39s\tremaining: 4.79s\n",
      "954:\tlearn: 0.3152272\ttotal: 1m 39s\tremaining: 4.69s\n",
      "955:\tlearn: 0.3151753\ttotal: 1m 39s\tremaining: 4.58s\n",
      "956:\tlearn: 0.3151023\ttotal: 1m 39s\tremaining: 4.48s\n",
      "957:\tlearn: 0.3150467\ttotal: 1m 39s\tremaining: 4.38s\n",
      "958:\tlearn: 0.3149225\ttotal: 1m 39s\tremaining: 4.27s\n",
      "959:\tlearn: 0.3148780\ttotal: 1m 40s\tremaining: 4.17s\n",
      "960:\tlearn: 0.3147739\ttotal: 1m 40s\tremaining: 4.06s\n",
      "961:\tlearn: 0.3146975\ttotal: 1m 40s\tremaining: 3.96s\n",
      "962:\tlearn: 0.3146255\ttotal: 1m 40s\tremaining: 3.85s\n",
      "963:\tlearn: 0.3145777\ttotal: 1m 40s\tremaining: 3.75s\n",
      "964:\tlearn: 0.3145132\ttotal: 1m 40s\tremaining: 3.65s\n",
      "965:\tlearn: 0.3144508\ttotal: 1m 40s\tremaining: 3.54s\n",
      "966:\tlearn: 0.3143916\ttotal: 1m 40s\tremaining: 3.44s\n",
      "967:\tlearn: 0.3143213\ttotal: 1m 40s\tremaining: 3.33s\n",
      "968:\tlearn: 0.3142757\ttotal: 1m 40s\tremaining: 3.23s\n",
      "969:\tlearn: 0.3142023\ttotal: 1m 41s\tremaining: 3.12s\n",
      "970:\tlearn: 0.3141471\ttotal: 1m 41s\tremaining: 3.02s\n",
      "971:\tlearn: 0.3140647\ttotal: 1m 41s\tremaining: 2.92s\n",
      "972:\tlearn: 0.3140055\ttotal: 1m 41s\tremaining: 2.81s\n",
      "973:\tlearn: 0.3139262\ttotal: 1m 41s\tremaining: 2.71s\n",
      "974:\tlearn: 0.3138729\ttotal: 1m 41s\tremaining: 2.6s\n",
      "975:\tlearn: 0.3138208\ttotal: 1m 41s\tremaining: 2.5s\n",
      "976:\tlearn: 0.3137873\ttotal: 1m 41s\tremaining: 2.4s\n",
      "977:\tlearn: 0.3135598\ttotal: 1m 41s\tremaining: 2.29s\n",
      "978:\tlearn: 0.3135388\ttotal: 1m 41s\tremaining: 2.19s\n",
      "979:\tlearn: 0.3134871\ttotal: 1m 42s\tremaining: 2.08s\n",
      "980:\tlearn: 0.3134388\ttotal: 1m 42s\tremaining: 1.98s\n",
      "981:\tlearn: 0.3133547\ttotal: 1m 42s\tremaining: 1.87s\n",
      "982:\tlearn: 0.3132698\ttotal: 1m 42s\tremaining: 1.77s\n",
      "983:\tlearn: 0.3132071\ttotal: 1m 42s\tremaining: 1.67s\n",
      "984:\tlearn: 0.3131755\ttotal: 1m 42s\tremaining: 1.56s\n",
      "985:\tlearn: 0.3131222\ttotal: 1m 42s\tremaining: 1.46s\n",
      "986:\tlearn: 0.3130895\ttotal: 1m 42s\tremaining: 1.35s\n",
      "987:\tlearn: 0.3130518\ttotal: 1m 42s\tremaining: 1.25s\n",
      "988:\tlearn: 0.3130059\ttotal: 1m 42s\tremaining: 1.15s\n",
      "989:\tlearn: 0.3129434\ttotal: 1m 43s\tremaining: 1.04s\n",
      "990:\tlearn: 0.3128894\ttotal: 1m 43s\tremaining: 937ms\n",
      "991:\tlearn: 0.3128633\ttotal: 1m 43s\tremaining: 833ms\n",
      "992:\tlearn: 0.3128079\ttotal: 1m 43s\tremaining: 729ms\n",
      "993:\tlearn: 0.3127712\ttotal: 1m 43s\tremaining: 624ms\n",
      "994:\tlearn: 0.3127244\ttotal: 1m 43s\tremaining: 520ms\n",
      "995:\tlearn: 0.3126446\ttotal: 1m 43s\tremaining: 416ms\n",
      "996:\tlearn: 0.3125820\ttotal: 1m 43s\tremaining: 312ms\n",
      "997:\tlearn: 0.3124423\ttotal: 1m 43s\tremaining: 208ms\n",
      "998:\tlearn: 0.3124162\ttotal: 1m 43s\tremaining: 104ms\n",
      "999:\tlearn: 0.3123639\ttotal: 1m 44s\tremaining: 0us\n",
      "[[30616  4112]\n",
      " [ 5820 29075]]\n",
      "0.8573459919854072\n",
      "CPU times: user 12min 43s, sys: 13 s, total: 12min 56s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#CatB\n",
    "X_trn, X_tst, Y_trn, Y_tst = train_test_split(X, Y, test_size=.3)\n",
    "\n",
    "model = cat.CatBoostClassifier()\n",
    "model.fit(X_trn, Y_trn)\n",
    "y_pred = model.predict(X_tst)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(Y_tst, y_pred))\n",
    "print(accuracy_score(Y_tst, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790b93d",
   "metadata": {
    "id": "JQ4M8Znd4_ep"
   },
   "source": [
    "# Model Exploration - Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b543d",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91699e6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5SphZ3ZMCrS",
    "outputId": "7ec80ce6-b978-42a2-d12f-ac2f0f8db335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fa7d3",
   "metadata": {},
   "source": [
    "## Train/Validation Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a760b4a",
   "metadata": {
    "id": "XiXAUWIwpO3I",
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "df = sc\n",
    "train_df, val_df = train_test_split(df,\n",
    "                                    test_size=0.3,\n",
    "                                    stratify=df.label,\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7e6a9",
   "metadata": {},
   "source": [
    "## Define BertTokenizer and Data loader (Set truncation for long text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f834b063",
   "metadata": {
    "id": "svAGWMoyr-rF"
   },
   "outputs": [],
   "source": [
    "class MNLIDataBert(Dataset):\n",
    "    def __init__(self, train_df, val_df):\n",
    "        self.label_dict = {0: 0, 1: 1}\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "\n",
    "        self.base_path = '/content/'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                                       do_lower_case=True,\n",
    "                                                       truncation=True)\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        self.init_data()\n",
    "\n",
    "    def init_data(self):\n",
    "        # Saving takes too much RAM\n",
    "        #\n",
    "        # if os.path.exists(os.path.join(self.base_path, 'train_data.pkl')):\n",
    "        #   print(\"Found training data\")\n",
    "        #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'rb') as f:\n",
    "        #     self.train_data = pickle.load(f)\n",
    "        # else:\n",
    "        #   self.train_data = self.load_data(self.train_df)\n",
    "        #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'wb') as f:\n",
    "        #     pickle.dump(self.train_data, f)\n",
    "        # if os.path.exists(os.path.join(self.base_path, 'val_data.pkl')):\n",
    "        #   print(\"Found val data\")\n",
    "        #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'rb') as f:\n",
    "        #     self.val_data = pickle.load(f)\n",
    "        # else:\n",
    "        #   self.val_data = self.load_data(self.val_df)\n",
    "        #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'wb') as f:\n",
    "        #     pickle.dump(self.val_data, f)\n",
    "        self.train_data = self.load_data(self.train_df)\n",
    "        self.val_data = self.load_data(self.val_df)\n",
    "\n",
    "    def load_data(self, df):\n",
    "        MAX_LEN = 50\n",
    "        token_ids = []\n",
    "        mask_ids = []\n",
    "        seg_ids = []\n",
    "        y = []\n",
    "\n",
    "        premise_list = df['lemm_Text'].to_list()\n",
    "        label_list = df['label'].to_list()\n",
    "\n",
    "        for (premise, label) in zip(premise_list, label_list):\n",
    "            premise_id = self.tokenizer.encode(premise,\n",
    "                                               add_special_tokens=True,\n",
    "                                               truncation=True,\n",
    "                                               max_length=50)\n",
    "            pair_token_ids = [self.tokenizer.cls_token_id\n",
    "                              ] + premise_id + [self.tokenizer.sep_token_id]\n",
    "            premise_len = len(premise_id)\n",
    "\n",
    "            segment_ids = torch.tensor(\n",
    "                [0] * (premise_len + 2))  # sentence 0 and sentence 1\n",
    "            attention_mask_ids = torch.tensor(\n",
    "                [1] * (premise_len + 2))  # mask padded values\n",
    "\n",
    "            token_ids.append(torch.tensor(pair_token_ids))\n",
    "            seg_ids.append(segment_ids)\n",
    "            mask_ids.append(attention_mask_ids)\n",
    "            y.append(self.label_dict[label])\n",
    "\n",
    "        token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "        mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "        seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "        y = torch.tensor(y)\n",
    "        dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "        print((len(dataset)))\n",
    "        return dataset\n",
    "\n",
    "    def get_data_loaders(self, batch_size=8, shuffle=True):\n",
    "        train_loader = DataLoader(self.train_data,\n",
    "                                  shuffle=shuffle,\n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "        val_loader = DataLoader(self.val_data,\n",
    "                                shuffle=shuffle,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "        return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ea3863b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "md52P1z14_e_",
    "outputId": "5af2c92c-a0c0-4264-f39b-4718d55ba4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162447\n",
      "69621\n"
     ]
    }
   ],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df7bb644",
   "metadata": {
    "id": "-LSyABLG4_fA"
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader = mnli_dataset.get_data_loaders(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a205c60",
   "metadata": {},
   "source": [
    "## Load Bert from pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c595b84d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOdc4Cs2DEjt",
    "outputId": "4e4e2f6c-e934-470d-f280-f4d60dd3ea00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62511969",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "640f3a2f",
   "metadata": {
    "id": "jxzpENXlEh-u"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [{\n",
    "    'params':\n",
    "    [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "    'weight_decay_rate':\n",
    "    0.01\n",
    "}, {\n",
    "    'params':\n",
    "    [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "    'weight_decay_rate':\n",
    "    0.0\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9646790",
   "metadata": {},
   "source": [
    "## Optimizer - AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d52bce89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "is1TqwTREid9",
    "outputId": "4988813d-e5bf-43e0-dacc-f18038238106"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79137d47",
   "metadata": {},
   "source": [
    "## Number of Trainable Parameters - 109,483,778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f7fa131",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79aI1dun4_fB",
    "outputId": "fd248f6f-1e2c-4fff-97ea-03468c5ec71f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,483,778 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93878ff7",
   "metadata": {},
   "source": [
    "## Define train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f31d5d9",
   "metadata": {
    "id": "qhU855Cw4_fB"
   },
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1)\n",
    "           == y_test).sum().float() / float(y_test.size(0))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14a593e8",
   "metadata": {
    "id": "OfhYO7Db4_fB"
   },
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss().to(device)\n",
    "EPOCHS = 3\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer):\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_acc = 0\n",
    "        for batch_idx, (pair_token_ids, mask_ids, seg_ids,\n",
    "                        y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            pair_token_ids = pair_token_ids.to(device)\n",
    "            mask_ids = mask_ids.to(device)\n",
    "            seg_ids = seg_ids.to(device)\n",
    "            labels = y.to(device)\n",
    "            # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "            loss, prediction = list(\n",
    "                model(pair_token_ids,\n",
    "                      token_type_ids=seg_ids,\n",
    "                      attention_mask=mask_ids,\n",
    "                      labels=labels).values())\n",
    "\n",
    "            # loss = criterion(prediction, labels.view(-1, 1))\n",
    "            acc = multi_acc(prediction, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_acc += acc.item()\n",
    "\n",
    "        train_acc = total_train_acc / len(train_loader)\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "        model.eval()\n",
    "        total_val_acc = 0\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (pair_token_ids, mask_ids, seg_ids,\n",
    "                            y) in enumerate(val_loader):\n",
    "                optimizer.zero_grad()\n",
    "                pair_token_ids = pair_token_ids.to(device)\n",
    "                mask_ids = mask_ids.to(device)\n",
    "                seg_ids = seg_ids.to(device)\n",
    "                labels = y.to(device)\n",
    "\n",
    "                # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "                loss, prediction = list(\n",
    "                    model(pair_token_ids,\n",
    "                          token_type_ids=seg_ids,\n",
    "                          attention_mask=mask_ids,\n",
    "                          labels=labels).values())\n",
    "\n",
    "                # loss = criterion(prediction, labels.view(-1, 1))\n",
    "                acc = multi_acc(prediction, labels)\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                total_val_acc += acc.item()\n",
    "\n",
    "        val_acc = total_val_acc / len(val_loader)\n",
    "        val_loss = total_val_loss / len(val_loader)\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end - start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}'\n",
    "        )\n",
    "        print((\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes),\n",
    "                                               seconds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152ead9",
   "metadata": {},
   "source": [
    "## Result - Accuracy 97.4% (28 min 23 s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76a49ea1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPIxT4RB4_fC",
    "outputId": "687cda29-04a4-499a-e357-0c3bc01261f0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.1015 train_acc: 0.9620 | val_loss: 0.0754 val_acc: 0.9740\n",
      "00:28:23.19\n",
      "Epoch 2: train_loss: 0.0511 train_acc: 0.9817 | val_loss: 0.0768 val_acc: 0.9724\n",
      "00:28:22.07\n",
      "Epoch 3: train_loss: 0.0274 train_acc: 0.9906 | val_loss: 0.0907 val_acc: 0.9713\n",
      "00:28:22.96\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b322f0",
   "metadata": {},
   "source": [
    "# Code Resource (Bert)\n",
    "\n",
    "https://github.com/dh1105/Sentence-Entailment/blob/main/Sentence_Entailment_BERT.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
